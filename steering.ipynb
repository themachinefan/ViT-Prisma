{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x13cfbb59960>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "import einops\n",
    "from vit_prisma.utils.data_utils.imagenet_utils import setup_imagenet_paths\n",
    "from vit_prisma.dataloaders.imagenet_dataset import ImageNetValidationDataset\n",
    "from vit_prisma.transforms.open_clip_transforms import get_clip_val_transforms\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from vit_prisma.sae.sae import SparseAutoencoder\n",
    "from vit_prisma.sae.sae_utils import download_sae_from_huggingface\n",
    "from vit_prisma.sae.evals.evals import EvalConfig\n",
    "from transformers import CLIPModel \n",
    "from transformers import CLIPProcessor\n",
    "from typing import List \n",
    "import urllib.request\n",
    "from fancy_einsum import einsum\n",
    "import torchvision\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_imagenet_val_dataset(dataset_path):\n",
    "\n",
    "    data_transforms = get_clip_val_transforms()\n",
    "    imagenet_paths = setup_imagenet_paths(dataset_path)\n",
    "    return ImageNetValidationDataset(imagenet_paths['val'], \n",
    "                                    imagenet_paths['label_strings'], \n",
    "                                    imagenet_paths['val_labels'], \n",
    "                                    data_transforms, return_index=True\n",
    "    )\n",
    "\n",
    "def get_imagenet_val_dataset_visualize(dataset_path):\n",
    "    imagenet_paths = setup_imagenet_paths(dataset_path)\n",
    "\n",
    "    return ImageNetValidationDataset(imagenet_paths['val'], \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'],\n",
    "                                torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "def get_clip_model(model_name='open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K'):                \n",
    "    model = HookedViT.from_pretrained(model_name, is_timm=False, is_clip=True, fold_ln=False, center_writing_weights=False) \n",
    "    return model\n",
    "\n",
    "def load_sae(download_dir):\n",
    "    repo_name = 'soniajoseph/updated-sae-weights'\n",
    "    file_id = 'UPDATED-final_sae_group_wkcn_TinyCLIP-ViT-40M-32-Text-19M-LAION400M_blocks.9.hook_mlp_out_8192.pt'\n",
    "    download_sae_from_huggingface(repo_name, file_id, download_dir)\n",
    "\n",
    "    sae_path= os.path.join(download_dir, file_id)\n",
    "    sae = SparseAutoencoder(EvalConfig()).load_from_pretrained_legacy_saelens_v2(sae_path)  #TODO may need option to modify cfg\n",
    "    return sae \n",
    "\n",
    "class ClipTextStuff:\n",
    "\n",
    "    def __init__(self, model_name, device=\"cuda\"):\n",
    "\n",
    "        self.device= device \n",
    "\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "        full_clip_model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "        self.text_model = full_clip_model.text_model.to(device)\n",
    "        self.text_projection = full_clip_model.text_projection.to(device)\n",
    "        self.logit_scale = full_clip_model.logit_scale\n",
    "        with urllib.request.urlopen(\"https://raw.githubusercontent.com/yossigandelsman/clip_text_span/main/text_descriptions/image_descriptions_general.txt\") as response:\n",
    "            self.labels = response.read().decode('utf-8').split('\\n')\n",
    "\n",
    "        self.labels_projected = self.get_text_embeds(self.labels)\n",
    "\n",
    "        # for context here is how similarity is computed:\n",
    "        # image_embeds = vision_outputs[1]\n",
    "        # image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # text_embeds = text_outputs[1]\n",
    "        # text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # # normalized features\n",
    "        # image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "        # text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        # # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        # logits_per_image = logits_per_text.t()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_text_embeds(self, list_of_strings:List[str]):\n",
    "        labels_input = self.clip_processor(text=list_of_strings, return_tensors='pt',  padding=True).input_ids\n",
    "        labels_input = labels_input.to(self.device)\n",
    "\n",
    "        labels_projections = self.text_projection(self.text_model(labels_input)[1])\n",
    "\n",
    "        # normalize\n",
    "        labels_projections = labels_projections/ labels_projections.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        return labels_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to: F:\\ViT-Prisma_fork\\data\\textspans\\attempted_models\\new\\UPDATED-final_sae_group_wkcn_TinyCLIP-ViT-40M-32-Text-19M-LAION400M_blocks.9.hook_mlp_out_8192.pt\n",
      "File size: 33592403 bytes\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 15869\n",
      "Total training images: 1300000\n",
      "Total wandb updates: 158\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 15 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "n_tokens_per_buffer (millions): 0.0512\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 15869\n",
      "Total training images: 1300000\n",
      "Total wandb updates: 158\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 15 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "Official model name wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\vit-prisma\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "output_folder = r\"F:/ViT-Prisma_fork/data/textspans/output\"\n",
    "imagenet_dataset_path = r\"F:/prisma_data/imagenet-object-localization-challenge\"  \n",
    "sae_download_dir =   r'F:/ViT-Prisma_fork/data/textspans/attempted_models/new'\n",
    "\n",
    "model_name = \"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\"\n",
    "\n",
    "sae = load_sae(sae_download_dir)\n",
    "model = get_clip_model(model_name)\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "hook_layer = 9\n",
    "hook_point = f\"blocks.{hook_layer}.hook_mlp_out\"\n",
    "#F:\\ViT-Prisma_fork\\data\\vision_sae_checkpoints\\comparisons\\mlp\\mlp_16\n",
    "feature_ids = [2381, 6405, 5379, 6827, 3471, 1436, 264, 6999, 1554, 2498, 4285 ]\n",
    "descriptions = [\"mustache\", \"bike\", \"food market (CLS TOKEN!)\", \"black and white photo (CLS TOKEN!)\",\n",
    "                 \"computer hardware (CLS TOKEN!)\", \"image boundaries\", \"The text 'in' and 'on'\",\n",
    "                 \"Polysemantic? animals in trees and certain products in foreground with plain backgrounds\",\n",
    "                 \"Chess?\",\n",
    "                 \"Sport players? Orange jerseys?\",\n",
    "                 \"Legs of sport players\",\n",
    "                 \n",
    "                 ]\n",
    "cls_token_or_not = [False, False, True, True, True, False, False, False, False, False,False]\n",
    "\n",
    "assert len(feature_ids) == len(descriptions) == len(cls_token_or_not)\n",
    "\n",
    "stop_at_layer = hook_layer+1 \n",
    "\n",
    "\n",
    "dataset = get_imagenet_val_dataset(imagenet_dataset_path)\n",
    "visualize_dataset = get_imagenet_val_dataset_visualize(imagenet_dataset_path)\n",
    "\n",
    "\n",
    "\n",
    "n_ctx = 50 \n",
    "\n",
    "device= \"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "sae = sae.to(device)\n",
    "clip_text_stuff = ClipTextStuff(model_name,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steering hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(x, hook, sparse_autoencoder, feature, amount):\n",
    "\n",
    "\n",
    "    reconstruction = sparse_autoencoder(x)[0]\n",
    "    error = x - reconstruction\n",
    "\n",
    "    boosted_feature_acts = sparse_autoencoder.encode_standard(x)\n",
    "    \n",
    "    #boosted_feature_acts[:,:,feature] = amount\n",
    "    #boosted_feature_acts[:,:,feature] =  boosted_feature_acts[:,:,feature] *amount\n",
    "    boosted_feature_acts[:,0,feature] = amount  \n",
    "    boosted_sae_out = einops.einsum(\n",
    "            boosted_feature_acts,\n",
    "            sparse_autoencoder.W_dec,\n",
    "            \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "        ) + sparse_autoencoder.b_dec\n",
    "    \n",
    "\n",
    "    boosted_sae_out = sparse_autoencoder.run_time_activation_norm_fn_out(boosted_sae_out)\n",
    "\n",
    "    return boosted_sae_out + error\n",
    "\n",
    "def run_model_with_steering(images,feature,amount ):\n",
    "    return F.normalize(model.run_with_hooks(\n",
    "                images,\n",
    "                fwd_hooks=[\n",
    "                    (hook_point, partial(steering_hook, sparse_autoencoder=sae, feature=feature, amount=amount))\n",
    "                ],\n",
    "                clear_contexts=True\n",
    "            ), p=1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"mustache\", \"facial hair\", \"dog\", \"cat\", \"sports\", \"black and white\", \"past\", \"40s\", \"photo\", \"computer\", \"hardware\", \"market\" ,\"crowd\", \n",
    "          \"bikes\", \"cycle\", \"giraffe\", \"sports\", \"organge\", \"collage\", \"mix up\", \"cut\", \"boundary\", \"chess\", \"knees\", \"legs\",\n",
    "          \"animals in tree\", \"trees\", \"animals\", \"in\", \"on\", \"text\"]\n",
    "\n",
    "labels_projected = clip_text_stuff.get_text_embeds(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple steering experiments. We'll see which labels get the biggest boost? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "random_batch = next(iter(dataloader))\n",
    "random_batch = random_batch[0].to(device)\n",
    "\n",
    "random_output = model(random_batch)\n",
    "\n",
    "random_output = F.normalize(random_output, p=2, dim=-1)\n",
    "\n",
    "default_scores = einsum( \"I D, T D -> I T\", random_output, clip_text_stuff.labels_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE 2381 HUMAN DESCRIPTION mustache\n",
      "0. Detailed illustration\n",
      "1. A gem\n",
      "2. A painting\n",
      "3. classic fine art piece\n",
      "4. An elegant photo\n",
      "5. Playful juxtaposition\n",
      "6. Timeless fine art piece\n",
      "7. A photo with a texture of mammals\n",
      "8. Striking juxtaposition\n",
      "9. Playful scenes\n",
      "FEATURE 6405 HUMAN DESCRIPTION bike\n",
      "0. An elegant photo\n",
      "1. A boot\n",
      "2. A gem\n",
      "3. A floor\n",
      "4. A zoomed out photo\n",
      "5. A beautiful photo\n",
      "6. An animal\n",
      "7. A low-resolution image\n",
      "8. Ocean\n",
      "9. A leg\n",
      "FEATURE 5379 HUMAN DESCRIPTION food market (CLS TOKEN!)\n",
      "0. A leg\n",
      "1. A close-up shot\n",
      "2. A gem\n",
      "3. A badge\n",
      "4. A low-resolution image\n",
      "5. A high-resolution image\n",
      "6. Close-up view\n",
      "7. Ocean\n",
      "8. An animal\n",
      "9. A portrait\n",
      "FEATURE 6827 HUMAN DESCRIPTION black and white photo (CLS TOKEN!)\n",
      "0. A leg\n",
      "1. A zoomed out photo\n",
      "2. A gem\n",
      "3. Ocean\n",
      "4. A low-resolution image\n",
      "5. A zoomed in photo\n",
      "6. A photograph of a big object\n",
      "7. Posed shot\n",
      "8. A close-up shot\n",
      "9. A beautiful photo\n",
      "FEATURE 3471 HUMAN DESCRIPTION computer hardware (CLS TOKEN!)\n",
      "0. A beautiful photo\n",
      "1. Ocean\n",
      "2. Photo that is taken outdoors\n",
      "3. An elegant photo\n",
      "4. A garden\n",
      "5. A gem\n",
      "6. A leg\n",
      "7. An aesthetic photo\n",
      "8. A fan\n",
      "9. A jacket\n",
      "FEATURE 1436 HUMAN DESCRIPTION image boundaries\n",
      "0. A leg\n",
      "1. A gem\n",
      "2. A zoomed out photo\n",
      "3. A boot\n",
      "4. mist\n",
      "5. A close-up shot\n",
      "6. A beautiful photo\n",
      "7. Ocean\n",
      "8. An animal\n",
      "9. A gold color\n",
      "FEATURE 264 HUMAN DESCRIPTION The text 'in' and 'on'\n",
      "0. Ocean\n",
      "1. A gem\n",
      "2. mist\n",
      "3. A leg\n",
      "4. fog\n",
      "5. desert vista\n",
      "6. Nostalgic vibe\n",
      "7. Close-up view\n",
      "8. Eyes\n",
      "9. A zoomed out photo\n",
      "FEATURE 6999 HUMAN DESCRIPTION Polysemantic? animals in trees and certain products in foreground with plain backgrounds\n",
      "0. A gem\n",
      "1. Posed shot\n",
      "2. Ocean\n",
      "3. A leg\n",
      "4. An elegant photo\n",
      "5. A low-resolution image\n",
      "6. mist\n",
      "7. A beautiful photo\n",
      "8. A group photo\n",
      "9. A high-resolution image\n",
      "FEATURE 1554 HUMAN DESCRIPTION Chess?\n",
      "0. A high-resolution image\n",
      "1. A low-resolution image\n",
      "2. Posed shot\n",
      "3. A gem\n",
      "4. A zoomed in photo\n",
      "5. A zoomed out photo\n",
      "6. A close-up shot\n",
      "7. A beautiful photo\n",
      "8. Photo featuring a lively festival\n",
      "9. A photo with motion blur\n",
      "FEATURE 2498 HUMAN DESCRIPTION Sport players? Orange jerseys?\n",
      "0. A beautiful photo\n",
      "1. A zoomed out photo\n",
      "2. A zoomed in photo\n",
      "3. Close-up view\n",
      "4. Photograph taken in a sunny weather\n",
      "5. A close-up shot\n",
      "6. Picture taken in a sunny day\n",
      "7. A photo taken in the summer\n",
      "8. Daytime shot\n",
      "9. Photo taken at noon\n",
      "FEATURE 4285 HUMAN DESCRIPTION Legs of sport players\n",
      "0. Ocean\n",
      "1. A beautiful photo\n",
      "2. fog\n",
      "3. A gem\n",
      "4. A leg\n",
      "5. mist\n",
      "6. Eyes\n",
      "7. Aesthetic pleasure\n",
      "8. A shirt\n",
      "9. Posed shot\n"
     ]
    }
   ],
   "source": [
    "boost_amount = 50\n",
    "top_k=10\n",
    "\n",
    "for i, feature_id in enumerate(feature_ids):\n",
    "   # if not cls_token_or_not[i]:\n",
    "   #     continue\n",
    "    print(\"FEATURE\", feature_id, \"HUMAN DESCRIPTION\", descriptions[i])\n",
    "    outputs = run_model_with_steering(random_batch, feature_id,boost_amount)\n",
    "\n",
    "\n",
    "\n",
    "    scores = einsum( \"I D, T D -> I T\", outputs,  clip_text_stuff.labels_projected)\n",
    "\n",
    "    diff = scores - default_scores\n",
    "\n",
    "    avg_diffs = torch.mean(scores, dim=0) \n",
    "    top_k_values, top_k_indices = torch.topk(avg_diffs, top_k, dim=0)\n",
    "   # max_diffs = torch.max(scores, dim=0)[0]\n",
    "   # top_k_values, top_k_indices = torch.topk(max_diffs, top_k, dim=0)\n",
    "    for j, ind in enumerate(top_k_indices):\n",
    "        ind = ind.item()\n",
    "        print(f\"{j}. {clip_text_stuff.labels[ind]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
