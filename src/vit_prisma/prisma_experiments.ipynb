{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "import base64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766c88e",
   "metadata": {},
   "source": [
    "#### change this stuff to fit your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beda781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "########### MODIFY THIS STUFF AS NEEDED!\n",
    "device = \"cuda\"\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "seed = 42\n",
    "\n",
    "MAX=500 # number of images to run on\n",
    "\n",
    "use_image_samples = True # uses image sample indices path (otherwise just use all imagenet)\n",
    "use_random_image_samples = False # uses random (up to seed) sample instead \n",
    "\n",
    "save_activations = False\n",
    "\n",
    "layer_nums = [7, 8, 9] # must be 0-9, the examples below assume their respective layers are specified up here\n",
    "\n",
    "from path_util import data_path, my_draft_folder, imagenet_path # this is something on my machine, remove and replace the following with your own paths\n",
    "file_name = 'mlp_fc1_{0}.npz'\n",
    "parquet_file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "json_file_path = os.path.join(data_path,'imagenet_class_index.json')\n",
    "\n",
    "image_sample_indices_path = os.path.join(my_draft_folder, 'imagenet_sample_indices.npy') # set to None to not use\n",
    "\n",
    "\n",
    "output_folder = os.path.join(my_draft_folder, 'outputs')\n",
    "\n",
    "gpt_output_folder = os.path.join(output_folder, \"gpt\")\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(gpt_output_folder, exist_ok=True)\n",
    "############################################################\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a555702",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "BATCH_INDEX = 'batch_index'\n",
    "IMAGE_INDEX = 'image_index'\n",
    "GT_CLASS = 'gt_class'\n",
    "PRED_CLASS = 'pred_class'\n",
    "PATCH_INDEX = 'patch_index'\n",
    "NEURON_INDEX = 'neuron_index'\n",
    "ACTIVATION_VALUE = 'activation'\n",
    "\n",
    "# Load the JSON file into a Python dictionary\n",
    "with open(json_file_path, 'r') as file:\n",
    "    num_to_word_dict = json.load(file)\n",
    "\n",
    "\n",
    "# Get class names\n",
    "imagenet_class_nums = np.arange(0, 1000, 1)\n",
    "imagenet_class_names = [\"{}\".format(num_to_word_dict.get(str(i), [\"Unknown label\"])[1]) for i in imagenet_class_nums]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b45b2",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3378c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "imagenet_data = datasets.ImageFolder(imagenet_path, transform=data_transforms)\n",
    "if use_image_samples:\n",
    "    \n",
    "    if not use_random_image_samples:\n",
    "        image_samples = np.load(image_sample_indices_path)\n",
    "    else: \n",
    "        image_samples = np.random.choice(len(imagenet_path), MAX, replace=False)\n",
    "\n",
    "    \n",
    "    imagenet_data = Subset(imagenet_data, image_samples)\n",
    "\n",
    "class ReturnIndexDatasetWrapper(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[index]\n",
    "        return img, label ,index \n",
    "    \n",
    "\n",
    "imagenet_data = ReturnIndexDatasetWrapper(imagenet_data)    \n",
    "data_loader = DataLoader(imagenet_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "imagestest, labelstest, index = next(iter(data_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05638204",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClipWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, imagenet_class_names, device):\n",
    "        super(ClipWrapper, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(\"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\")\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\", do_rescale=False) # Make sure the do_rescale is false for pytorch datasets\n",
    "        self.imagenet_class_names = imagenet_class_names\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #inputs  =self.processor(text=\"CAR CAR\", images=x, return_tensors=\"pt\", padding=True)\n",
    "        inputs  =self.processor(text=self.imagenet_class_names, images=x, return_tensors=\"pt\", padding=True)\n",
    "        # processor returns cpu tensors even if input is cuda :/ Makes this whole wrapper ill-advised\n",
    "        # TODO processor should run in Dataset instead\n",
    "        for key in inputs.keys():\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        return  self.clip(**inputs)\n",
    "\n",
    "\n",
    "model = ClipWrapper(imagenet_class_names, device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# example of running model\n",
    "images, labels, image_indices = next(iter(data_loader))\n",
    "\n",
    "idx = 0\n",
    "\n",
    "outputs = model(images)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "predicted_indices = probs.argmax(dim=1)\n",
    "\n",
    "plt.imshow(images.cpu()[idx].permute(1, 2, 0))\n",
    "plt.title(f\"Predicted: {imagenet_class_names[predicted_indices[idx].item()]}. True: {imagenet_class_names[labels[idx].item()]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce3444",
   "metadata": {},
   "source": [
    "#### Get the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab51467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activations_list = []\n",
    "\n",
    "# Function to register the hook\n",
    "#TODO do multiple layers at once\n",
    "def register_hook(module, activations_list):\n",
    "    def hook(module, input, output):\n",
    "        activations_list.append(output.detach())\n",
    "    return module.register_forward_hook(hook)\n",
    "\n",
    "def process_images(model, total_images, total_labels, image_indices, batch_idx, detach=True, flatten=True):\n",
    "    activations_list.clear()\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    outputs = model(total_images)\n",
    "\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "    class_indices = probs.argmax(dim=1)\n",
    "    total_labels = total_labels.to(class_indices.device)\n",
    "\n",
    "    batch_activations = activations_list[0]\n",
    "\n",
    "   # TODO Much better than looping but maybe could be restructured entirely? (batch_idx, image_idx, class_name, predicted are all constants per batch moreoever they are int. patch_idx, neuron_idx could be infered from shape. Should probably just return activations + 'constants' and figure out the rest when trying to convert to panda?). Intstead I'm trying to mimic the exact format of the old list in a float tensor.... :O\n",
    "\n",
    "\n",
    "    b, p, n = batch_activations.shape\n",
    "    dt = batch_activations.dtype\n",
    "    dev = batch_activations.device\n",
    "\n",
    "    patch_index = torch.arange(p, dtype=dt).to(dev)\n",
    "    neuron_index = torch.arange(n, dtype=dt).to(dev)\n",
    "\n",
    "    patch_index, neuron_index = torch.meshgrid(patch_index, neuron_index, indexing='ij')\n",
    "    patch_index = repeat(patch_index, 'h w -> b h w 1', b=b)\n",
    "    neuron_index = repeat(neuron_index, 'h w -> b h w 1', b=b)\n",
    "\n",
    "    batch_indices = torch.full_like(neuron_index, fill_value=batch_idx)\n",
    "    image_indices = torch.tensor(image_indices, dtype=dt).to(dev)\n",
    "\n",
    "    image_indices = repeat(image_indices, 'b -> b h w 1', h=p, w=n)\n",
    "    gt_class_indices = repeat(total_labels, 'b -> b h w 1', h=p, w=n)\n",
    "    pred_class_indices =  repeat(class_indices, 'b -> b h w 1', h=p, w=n)\n",
    "\n",
    "\n",
    "    activations_table =torch.concatenate([batch_indices, image_indices,gt_class_indices,pred_class_indices,  patch_index, neuron_index,batch_activations.unsqueeze(-1)], dim=-1)\n",
    "    if flatten:\n",
    "        activations_table = rearrange(activations_table, 'b h w c -> (b h w) c')\n",
    "    if detach:\n",
    "        activations_table = activations_table.detach().cpu()\n",
    "    return activations_table\n",
    "\n",
    "def tensor_activations_to_panda(activations, verbose=True, convert_to_string=False):\n",
    "\n",
    "\n",
    "    \n",
    "    df_activations = pd.DataFrame(activations)\n",
    "\n",
    "\n",
    "\n",
    "    # Assign column names\n",
    "    df_activations.columns = [ BATCH_INDEX, IMAGE_INDEX, GT_CLASS, PRED_CLASS, PATCH_INDEX, NEURON_INDEX, ACTIVATION_VALUE ]\n",
    "\n",
    "    # convert everything but activation to int\n",
    "    for int_names in df_activations.columns[:-1]:\n",
    "        df_activations[int_names] = df_activations[int_names].astype(int)\n",
    "\n",
    "    # convert class indices to string names (a bit slow)\n",
    "    if convert_to_string:\n",
    "        if verbose:\n",
    "            print(\"converting to string...\")\n",
    "        #TODO only do this when actually displaying something\n",
    "        df_activations[GT_CLASS] = df_activations[GT_CLASS].map(lambda x: imagenet_class_names[x])\n",
    "        df_activations[PRED_CLASS] = df_activations[PRED_CLASS].map(lambda x: imagenet_class_names[x])\n",
    "        if verbose:\n",
    "            print(\"converting done.\") \n",
    "\n",
    "    return df_activations\n",
    "\n",
    "def process_images_to_panda(model, total_images, total_labels, image_indices, batch_idx, verbose=False, convert_to_string=False):\n",
    "    return tensor_activations_to_panda(process_images(model, total_images, total_labels, image_indices, batch_idx), verbose=verbose, convert_to_string=convert_to_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activations_per_layer={}\n",
    "sorted_activations_per_layer = {}\n",
    "for layer_num in layer_nums:\n",
    "\n",
    "    module = model.clip.vision_model.encoder.layers[layer_num].mlp.fc1 # Layer number here\n",
    "    hook_handle = register_hook(module, activations_list)\n",
    "\n",
    "    master_layer_activations = []\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for batch_idx, (total_images, total_labels, total_indices) in tqdm(enumerate(data_loader), total=MAX//batch_size):\n",
    "\n",
    "            detailed_activations = process_images(model, total_images, total_labels, total_indices, batch_idx=batch_idx)\n",
    "            master_layer_activations.append(detailed_activations)\n",
    "\n",
    "            count += batch_size\n",
    "            if count >= MAX:\n",
    "                break\n",
    "\n",
    "\n",
    "    # Remove the hook when done\n",
    "    hook_handle.remove()\n",
    "\n",
    "    activations = torch.cat(master_layer_activations, dim=0)\n",
    "\n",
    "\n",
    "    df_activations = tensor_activations_to_panda(activations)\n",
    "\n",
    "    print(\"sorting..\")\n",
    "    #TODO pytorch sort?\n",
    "    sorted = df_activations.sort_values(by=[ACTIVATION_VALUE], ascending=False, inplace=False)\n",
    "    print(\"sorting done\")\n",
    "\n",
    "    activations_per_layer[layer_num] = df_activations\n",
    "    sorted_activations_per_layer[layer_num] = sorted\n",
    "    if save_activations:\n",
    "        #TODO option to load\n",
    "        df_activations.to_parquet(parquet_file_path.format(layer_num), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff7756",
   "metadata": {},
   "source": [
    "### EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29da1c",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def plot_image_patch_heatmap(images, heatmaps, titles=None, main_title=\"\" ,width=4, alpha=0.4, save=None):\n",
    "    if titles is None:\n",
    "        titles = [''] * len(images)\n",
    "\n",
    "    assert len(images) == len(heatmaps) == len(titles), \"Images, heatmaps, and titles must have the same length.\"\n",
    "    \n",
    "    num_images = len(images)\n",
    "    height = int(np.ceil(num_images/width))\n",
    "    if height == 1:\n",
    "        width = num_images\n",
    "    fig, axes = plt.subplots(height, width, figsize=(5*width, 5*height))\n",
    "\n",
    "    fig.suptitle(main_title, fontsize=12)\n",
    "\n",
    "    if num_images == 1:\n",
    "        axes = np.array([[axes]])\n",
    "   \n",
    "\n",
    "    \n",
    "    for idx, (image, heatmap, title) in enumerate(zip(images, heatmaps, titles)):\n",
    "        ax = axes.flatten()[idx]\n",
    "        \n",
    "        image_size = image.shape[-1]\n",
    "        pixel_num = heatmap.shape[-1]\n",
    "\n",
    "        # Create a heatmap overlay\n",
    "        heatmap_expanded = np.zeros((image_size, image_size))\n",
    "        patch_size = image_size // pixel_num\n",
    "\n",
    "        for i in range(pixel_num):\n",
    "            for j in range(pixel_num):\n",
    "                heatmap_expanded[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = heatmap[i, j]\n",
    "\n",
    "        # Plotting the image with the heatmap overlay\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.imshow(heatmap_expanded, cmap='viridis', alpha=0.6)  # Overlaying the heatmap\n",
    "        ax.axis('off')  \n",
    "        \n",
    "        min_activation = heatmap.min()\n",
    "        max_activation = heatmap.max()\n",
    "\n",
    "\n",
    "\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Hide any remaining subplots that don't have data\n",
    "    for ax in axes.flatten()[num_images:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save is not None:\n",
    "        plt.savefig(save)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_activation_array(image_idx, neuron_idx, activations, pixel_num=14):\n",
    "\n",
    "    filtered_df = activations[(activations[IMAGE_INDEX] == image_idx) &\n",
    "                            (activations[NEURON_INDEX] == neuron_idx)]\n",
    "\n",
    "    activation_values = filtered_df[ACTIVATION_VALUE]\n",
    "\n",
    "    activation_values_array = activation_values.to_numpy()[1:]\n",
    "    activation_values_array = activation_values_array.reshape(pixel_num, pixel_num)\n",
    "\n",
    "    return activation_values_array\n",
    "\n",
    "def plot_activations_heatmap(image_indices, neuron_indices, titles, main_title=\"\", activations=df_activations, dataset=imagenet_data , pixel_num=14, alpha=0.4, save=None):\n",
    "    images = []\n",
    "    heatmaps = []\n",
    "    for image_idx, neuron_idx in zip(image_indices, neuron_indices):\n",
    "\n",
    "        \n",
    "        image = dataset[image_idx][0]\n",
    "\n",
    "\n",
    "        images.append(image)\n",
    "        heatmaps.append(get_activation_array(image_idx, neuron_idx, activations, pixel_num) )\n",
    "    \n",
    "    plot_image_patch_heatmap(images, heatmaps, titles, main_title=main_title, alpha=alpha, save=save)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_n = 10\n",
    "\n",
    "interesting_neurons = [\n",
    "    # Layer 9\n",
    "    (9, 145, 'Car. Details. Headlights and handles'),\n",
    "    (9, 327, 'Cars'),\n",
    "    (9, 398, 'Snowy white fluffy. Polysemantic.'),\n",
    "    (9, 496, 'Beach'),\n",
    "    (9, 179, 'Very polysemantic. faces, corners, coastal images, dogs...'),\n",
    "    (9, 469, 'Snowy white fluffy'),\n",
    "    (9, 659, 'Water, water animals'),\n",
    "    \n",
    "    # Layer 8\n",
    "    (8, 79, 'Palm trees and fronds'),\n",
    "    (8, 392, 'Curly text and designs'),\n",
    "    (8, 490, 'Flames / glinty light reflections'),\n",
    "    (8, 493, 'Text'),\n",
    "    (8, 539, 'Faces (male?)'),\n",
    "    (8, 699, 'Underwater backgrounds'),\n",
    "    (8, 893, 'Faces'),\n",
    "    (8, 927, 'Rims of hats, hats in general, hoods, and dog ears that may look like hats'),\n",
    "    \n",
    "    # Layer 7\n",
    "    (7, 107, 'Side of the face / neck'),\n",
    "    (7, 326, 'Text at corner of the image'),\n",
    "    (7, 370, 'Top-of-the-head'),\n",
    "    (7, 494, 'Pants/legs'),\n",
    "    (7, 649, 'Logo'),\n",
    "    (7, 670, 'Logo'),\n",
    "]\n",
    "\n",
    "#neuron_idx = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO could use speed ups from below\n",
    "for layer_num, neuron_idx, human_label in  interesting_neurons:\n",
    "\n",
    "    df_activations = activations_per_layer[layer_num]\n",
    "    df_sorted = sorted_activations_per_layer[layer_num]\n",
    "    #TODO why drop duplicates?\n",
    "    unique_top_entries = df_sorted[df_sorted[NEURON_INDEX] == neuron_idx].drop_duplicates(subset=GT_CLASS).head(top_n)\n",
    "\n",
    "    images, titles = [], []\n",
    "    for i, (image_index, class_name) in enumerate(zip(unique_top_entries[IMAGE_INDEX], unique_top_entries[GT_CLASS])):\n",
    "        title = f\"Image: {image_index}\\nClass: {class_name}\"\n",
    "        images.append(image_index)\n",
    "        titles.append(title)\n",
    "    main_title = f\"ACTIVATIONS\\nLayer/Neuron: {layer_num}/{neuron_idx}\\n{human_label}\"\n",
    "    plot_activations_heatmap(images, [neuron_idx]*len(images), titles, main_title=main_title, activations=df_activations, dataset=imagenet_data, save=os.path.join(output_folder, f\"{layer_num:02d}_{neuron_idx:04d}_activations.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc27ce",
   "metadata": {},
   "source": [
    "#### Finding key input patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is a lot of computation, likely your notebook won't handle all neurons at once..\n",
    "\n",
    "#TODO instead of zero-ing out a patch take a random patch from elsewhere?\n",
    "\n",
    "#TODO this could use a cleaner version of process_images since much of it isn't used\n",
    "\n",
    "# TODO currently assuming square\n",
    "num_patches_width = 14\n",
    "#interesting_neurons = [\n",
    "    # (7, 494, 'Pants/legs'),\n",
    "    # (7, 649, 'Logo'),\n",
    "    # (7, 670, 'Logo'),\n",
    "    #(7, 326, 'Text at corner of the image'),\n",
    "\n",
    "#]\n",
    "# TODO these functions not really used much anymore\n",
    "def image_to_batch(image):\n",
    "    torch_image = np.expand_dims(image,axis=0)\n",
    "    torch_image = torch.from_numpy(torch_image).to(device)\n",
    "    return torch_image\n",
    "def label_to_batch(label):\n",
    "    return torch.tensor([label]).to(device)\n",
    "#TODO some neurons share images so could do for both at once..\n",
    "def register_hook_specific_neuron(module, activations_list, neuron_idx):\n",
    "    def hook(module, input, output):\n",
    "        activations_list.append(output[:,:, neuron_idx].unsqueeze(-1).detach())\n",
    "    return module.register_forward_hook(hook)\n",
    "\n",
    "#master_data = []\n",
    "arange = torch.arange(batch_size).to(device)\n",
    "for layer_num, neuron_idx, human_label in  tqdm(interesting_neurons):\n",
    "\n",
    "    activations_list = []\n",
    "    module = model.clip.vision_model.encoder.layers[layer_num].mlp.fc1 \n",
    "    hook_handle = register_hook_specific_neuron(module, activations_list, neuron_idx)\n",
    "    dummy_neuron_idx = 0\n",
    "    df_activations = activations_per_layer[layer_num]\n",
    "    df_sorted = sorted_activations_per_layer[layer_num]\n",
    "\n",
    "    unique_top_entries = df_sorted[df_sorted[NEURON_INDEX] == neuron_idx].drop_duplicates(subset=GT_CLASS).head(top_n)\n",
    "\n",
    "    images, heatmaps, titles = [], [], []\n",
    "    for i, (image_index, class_name) in enumerate(zip(unique_top_entries[IMAGE_INDEX], unique_top_entries[GT_CLASS])):\n",
    "\n",
    "        title = f\"Image: {image_index}\\nClass: {class_name}\"\n",
    "        image, label = imagenet_data[image_index][0:2]\n",
    "        batch_image= image_to_batch(image)\n",
    "        batch_label = label_to_batch(label)\n",
    "        default_activations = process_images(model, batch_image, batch_label, [image_index], batch_idx=-1,detach=False, flatten=False)\n",
    "        default_activations = default_activations[0,:, dummy_neuron_idx,-1]\n",
    "\n",
    "        #default_activations = rearrange(default_activations[1:], '(x y)-> x y', x=num_patches_width, y=num_patches_width)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        patch_size = image.shape[-1] // num_patches_width\n",
    "\n",
    "        zero_patch_images = {} \n",
    "\n",
    "        #diffs = torch.zeros((num_patches_width, num_patches_width))\n",
    "        diffs = []\n",
    "        gathering_images = []\n",
    "        gathering_dummy_indices = []\n",
    "        \n",
    "        all_activations = []\n",
    "        count = 0\n",
    "        for i, j in np.ndindex((num_patches_width, num_patches_width)):\n",
    "            zeroed_image = image.clone()\n",
    "            zeroed_image[:,i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = 0\n",
    "\n",
    "            gathering_images.append(zeroed_image)\n",
    "            gathering_dummy_indices.append(i*num_patches_width + j)\n",
    "\n",
    "\n",
    "            if len(gathering_images) == batch_size or (i == num_patches_width-1 and j == num_patches_width - 1):\n",
    "\n",
    "                zeroed_torch_images = torch.stack(gathering_images, dim=0).to(device)\n",
    "\n",
    "          \n",
    "                zeroed_activations = process_images(model, zeroed_torch_images, torch.tensor([label]*len(gathering_images)).to(device), gathering_dummy_indices, batch_idx=-1, detach=False, flatten=False)\n",
    "                zeroed_activations = zeroed_activations[:,:, dummy_neuron_idx,-1]\n",
    "\n",
    "\n",
    "                # make square\n",
    "                #zeroed_activations = rearrange(zeroed_activations[:,1:], 'b (x y)-> b x y', x=num_patches_width, y=num_patches_width)\n",
    "\n",
    "                diff = torch.abs(zeroed_activations-default_activations)\n",
    "\n",
    "\n",
    "                # zero out the diff in which the input image is 0 since we expect it to be different\n",
    "                if diff.shape[0] == batch_size:\n",
    "                    r = arange\n",
    "                else:\n",
    "                    r =  torch.arange(diff.shape[0]).to(device)\n",
    "                diff[r, r +gathering_dummy_indices[0]] = 0 \n",
    "                \n",
    "                diff = diff.mean(dim=1)\n",
    "\n",
    "                diffs.append(diff)\n",
    "\n",
    "                gathering_images = []\n",
    "                gathering_dummy_indices = []\n",
    "\n",
    "\n",
    "\n",
    "        diff = torch.cat(diffs, dim=0)\n",
    "\n",
    "        diff = diff.reshape((num_patches_width, num_patches_width))\n",
    "        diff = diff.detach().cpu()\n",
    "        #plot_image_patch_heatmap([image], [diff], titles=['test'], main_title=\"\" ,width=4, alpha=0.4)\n",
    "\n",
    "        images.append(image)\n",
    "        heatmaps.append(diff)\n",
    "        titles.append(title)\n",
    "\n",
    "    main_title = f\"INPUT SIGNIFICANCE\\nLayer/Neuron: {layer_num}/{neuron_idx}\\n{human_label}\"\n",
    "    plot_image_patch_heatmap(images, heatmaps, titles, main_title=main_title, save=os.path.join(output_folder, f\"{layer_num:02d}_{neuron_idx:04d}_input_significance.jpg\"))\n",
    "\n",
    "    #master_data.append((images,heatmaps, titles, main_title))\n",
    "    hook_handle.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b4967",
   "metadata": {},
   "source": [
    "### Asking GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f332f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We assume OPENAI_API_KEY is an env key you can set it here\n",
    "# os.environ['OPENAI_API_KEY'] = 'yourkey'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff63746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf stuff\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate,  Paragraph, Spacer\n",
    "from reportlab.platypus import Image as PDFImage\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from PIL import Image \n",
    "import io \n",
    "def resize_image(image, max_width, max_height):\n",
    "    # Calculate the new dimensions maintaining the aspect ratio\n",
    "    width_percent = max_width / float(image.size[0])\n",
    "    height_percent = max_height / float(image.size[1])\n",
    "    aspect_ratio = min(width_percent, height_percent)\n",
    "\n",
    "    # New dimensions\n",
    "    width = int((float(image.size[0]) * float(aspect_ratio)))\n",
    "    height = int((float(image.size[1]) * float(aspect_ratio)))\n",
    "\n",
    "    return image.resize((width, height), Image.ANTIALIAS)\n",
    "def create_pdf(content_list, file_name):\n",
    "    doc = SimpleDocTemplate(file_name, pagesize=letter)\n",
    "    elements = []\n",
    "    style_sheet = getSampleStyleSheet()\n",
    "\n",
    "    #TODO figure out how to do this better.. \n",
    "    max_image_width = 456#letter[0] - 50\n",
    "    max_image_height = 636#letter[1] - 50\n",
    "    for item in content_list:\n",
    "        if isinstance(item, str):\n",
    "            # Add text\n",
    "            item = item.replace('\\n', '<br/>')\n",
    "\n",
    "            elements.append(Paragraph(item, style_sheet['BodyText']))\n",
    "            elements.append(Spacer(1, 12))  # Add space after paragraph\n",
    "\n",
    "        elif isinstance(item, np.ndarray):\n",
    "            # Convert numpy array to list of lists and create a table\n",
    "            image = Image.fromarray(item.astype('uint8'), 'RGB')\n",
    "            if image.size[0] > max_image_width or image.size[1] > max_image_height:\n",
    "                image = resize_image(image, max_image_width, max_image_height)\n",
    "            image_buffer = io.BytesIO()\n",
    "            image.save(image_buffer, format='PNG')\n",
    "            image_buffer.seek(0)\n",
    "            img = PDFImage(image_buffer)\n",
    "            elements.append(img)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Content list must contain only strings and numpy arrays.\")\n",
    "\n",
    "    doc.build(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "# converts a numpy array image to encoded bytes\n",
    "def convert_to_base(vis:np.ndarray):\n",
    "    image = Image.fromarray(vis)\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")  \n",
    "    byte_data = buffer.getvalue()\n",
    "    return base64.b64encode(byte_data).decode('utf-8')\n",
    "\n",
    "# handler for messages to gpt-v\n",
    "class ImageMessages:\n",
    "    def __init__(self, json_output=False ):\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"\n",
    "          }\n",
    "\n",
    "        self.content = []\n",
    "\n",
    "        self.human_readable_content = []\n",
    "        self.init_content()\n",
    "\n",
    "    def init_content(self):\n",
    "        self.content = []\n",
    "        self.human_readable_content = []\n",
    "\n",
    "\n",
    "    def add_content_text(self,new_text):\n",
    "        self.content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": new_text})\n",
    "        self.human_readable_content.append(\"USER: \" + new_text)\n",
    "\n",
    "    def add_content_image(self, vis, detail=\"high\"):\n",
    "        self.content.append({\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{convert_to_base(vis)}\",\n",
    "                    \"detail\": detail,\n",
    "                  }})\n",
    "        self.human_readable_content.append(vis)\n",
    "\n",
    "    def add_comment(self, text):\n",
    "        self.human_readable_content.append(\"###COMMENT (not seen by gpt): \" + text + \"###\")\n",
    "        \n",
    "    def run_gpt(self):\n",
    "        payload = {\n",
    "            \"model\": \"gpt-4-vision-preview\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self.content\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "    \n",
    "    \n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=self.headers, json=payload)\n",
    "        text = response.json()['choices'][0]['message']['content']\n",
    "        self.human_readable_content.append(\"GPT: \" + text)\n",
    "        return text\n",
    "    \n",
    "    def convert_to_pdf(self, filename):\n",
    "        create_pdf(self.human_readable_content, filename)\n",
    "\n",
    "    def jupyter_notebook_display(self):\n",
    "        for blah in self.human_readable_content:\n",
    "            if type(blah) == str:\n",
    "                print(blah)\n",
    "            else:\n",
    "                plt.imshow(blah)\n",
    "                plt.axis('off') \n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6835c",
   "metadata": {},
   "source": [
    "#### Freeform\n",
    "Here I just ask gpt what it sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3869d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this test I'm just asking nicely\n",
    "\n",
    "#TODO pass along the patch activations in some form to gpt \n",
    "#TODO pass the labels to gpt \n",
    "#TODO pass examples so it's not zero shot(!)\n",
    "#TODO since these images are low resolution I think it's cheaper to send them individually but in general that isn't true so might want to concat. Concatenating them may effect model answers though!\n",
    "#here I'm concatenating mostly because gpt is very sensitive to prompt and this happens to work...\n",
    "#TODO infinite number of possible tweaks...\"\n",
    "\n",
    "\n",
    "\n",
    "interesting_neurons = [\n",
    "    # Layer 9\n",
    "    (9, 145, 'Car. Details. Headlights and handles'),\n",
    "    (9, 327, 'Cars'),\n",
    "    (9, 398, 'Snowy white fluffy. Polysemantic.'),\n",
    "    (9, 496, 'Beach'),\n",
    "    (9, 179, 'Very polysemantic. faces, corners, coastal images, dogs...'),\n",
    "    (9, 469, 'Snowy white fluffy'),\n",
    "    (9, 659, 'Water, water animals'),\n",
    "    \n",
    "    # Layer 8\n",
    "    (8, 79, 'Palm trees and fronds'),\n",
    "    (8, 392, 'Curly text and designs'),\n",
    "    (8, 490, 'Flames / glinty light reflections'),\n",
    "    (8, 493, 'Text'),\n",
    "    (8, 539, 'Faces (male?)'),\n",
    "    (8, 699, 'Underwater backgrounds'),\n",
    "    (8, 893, 'Faces'),\n",
    "    (8, 927, 'Rims of hats, hats in general, hoods, and dog ears that may look like hats'),\n",
    "    \n",
    "    # Layer 7\n",
    "    (7, 107, 'Side of the face / neck'),\n",
    "    (7, 326, 'Text at corner of the image'),\n",
    "    (7, 370, 'Top-of-the-head'),\n",
    "    (7, 494, 'Pants/legs'),\n",
    "    (7, 649, 'Logo'),\n",
    "    (7, 670, 'Logo'),\n",
    "]\n",
    "\n",
    "detail=\"auto\"\n",
    "#detail = \"low\"\n",
    "\n",
    "start_text = f\"\"\"The following images from imagenet produced high activations from a neuron in TinyClip. I'm trying to determine what common feature(s) the neuron is reacting to.\"\"\"\n",
    "\n",
    "messages = ImageMessages()\n",
    "hacky_max = None\n",
    "count = 0\n",
    "for layer_num, neuron_idx, human_label in  tqdm(interesting_neurons):\n",
    "\n",
    "    df_activations = activations_per_layer[layer_num]\n",
    "    df_sorted = sorted_activations_per_layer[layer_num]\n",
    "\n",
    "    unique_top_entries = df_sorted[df_sorted[NEURON_INDEX] == neuron_idx].drop_duplicates(subset=GT_CLASS).head(top_n)\n",
    "    messages.init_content()\n",
    "    messages.add_comment(f\"Layer/Neuron: {layer_num}/{neuron_idx} human answer: {human_label}\")\n",
    "\n",
    "    messages.add_content_text(start_text)\n",
    "    #TODO ignores requires if not given all together..\n",
    "    master_image = np.empty((0,imagenet_data[0][0].shape[1],3), dtype=np.uint8)\n",
    "    for i, (image_index,) in enumerate(zip(unique_top_entries[IMAGE_INDEX], )):\n",
    "\n",
    "        image = imagenet_data[image_index][0].cpu().numpy()\n",
    "        image = np.uint8(255*np.transpose(image, (1, 2, 0)))\n",
    "\n",
    "        master_image = np.concatenate([master_image,image],axis=0)\n",
    "        \n",
    "        # NOTE \n",
    "        #messages.add_content_text(f\"Image {i+1}\")\n",
    "        #messages.add_content_image(image, detail=detail)\n",
    "    messages.add_content_image(master_image, detail=detail)\n",
    "\n",
    "    messages.run_gpt()\n",
    "\n",
    "    messages.convert_to_pdf(os.path.join(gpt_output_folder, f\"{layer_num:02d}_{neuron_idx:04d}_freeform_answer.pdf\"))\n",
    "    messages.jupyter_notebook_display()\n",
    "         \n",
    "\n",
    "    count = count + 1\n",
    "    if hacky_max is not None and count >= hacky_max:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#TODO auto interpretability score!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d7ca8",
   "metadata": {},
   "source": [
    "#### Spearman rank correlation\n",
    "choice of samples here is very important. from the anthropics paper:\n",
    "\n",
    "Claude is asked to predict activations for sixty examples: six from the top activations; two from the other 12 intervals; ten completely random; and twenty top activating tokens out of context.\n",
    "\n",
    "Here I'm just arbitrarily taking 7 samples [0,3,6, 9, 12,15,18] and asking the model to rank [1, 4, 7 ...]\n",
    "I have no solid justification for this, In fact with this amount of data the task seems pointless.\n",
    "\n",
    "Moreoever this is not a good way to rank things (it doesn't scale well). Persumably they ask the model to score images instead of directly asking it to rank? I haven't actually checked.\n",
    "\n",
    "But I just want to write up some basic code and get a sense of what it does...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# see markdown above about sample\n",
    "example_samples_indices = [0,3,6, 9, 12,15,18]\n",
    "\n",
    "test_samples_indices = [n+1 for n in example_samples_indices] \n",
    "\n",
    "messages = ImageMessages(json_output=True)\n",
    "\n",
    "detail = \"auto\"\n",
    "\n",
    "start_text =  f\"\"\"The following {len(example_samples_indices)} images from imagenet have been ranked based on their activation value from a neuron in TinyClip.\"\"\"\n",
    "question_text = f\"\"\"Here are {len(test_samples_indices)} more images. Based on the above ranking try to predict the rank each image. Each image has a name, A, B, C...\n",
    "Format your answer as a json dict with two keys. Under key 'info', describe your thought process. Under key 'ranking' make a list of the names in order\"\"\"\n",
    "final_text = f\"Return your answer. Remember the json format\"\n",
    "\n",
    "hacky_max = None\n",
    "count = 0\n",
    "r_values = []\n",
    "p_values = []\n",
    "for layer_num, neuron_idx, human_label in  tqdm(interesting_neurons):\n",
    "\n",
    "    df_activations = activations_per_layer[layer_num]\n",
    "    df_sorted = sorted_activations_per_layer[layer_num]\n",
    "\n",
    "    #TODO I don't think it makes sense to drop classes for this but I just want to keep the examples the same as above\n",
    "    unique_sorted = df_sorted[df_sorted[NEURON_INDEX] == neuron_idx].drop_duplicates(subset=GT_CLASS)\n",
    "    example_samples = unique_sorted.iloc[example_samples_indices]\n",
    "    random_test_indices = random.sample(test_samples_indices, len(test_samples_indices))\n",
    "    test_samples = unique_sorted.iloc[random_test_indices]\n",
    "    messages.init_content()\n",
    "    messages.add_comment(f\"Layer/Neuron: {layer_num}/{neuron_idx} human answer: {human_label}\")\n",
    "\n",
    "    messages.add_content_text(start_text)\n",
    "\n",
    "    for i, (image_index, class_name, act_val) in enumerate(zip(example_samples[IMAGE_INDEX], example_samples[GT_CLASS], example_samples[ACTIVATION_VALUE])):\n",
    "        rank = i + 1\n",
    "        class_name =imagenet_class_names[class_name]\n",
    "        act_val = round(act_val, 2)\n",
    "        image = imagenet_data[image_index][0].cpu().numpy()\n",
    "        image = np.uint8(255*np.transpose(image, (1, 2, 0)))\n",
    "        \n",
    "        messages.add_content_text(f\"Rank: {rank}. Class name: {class_name}. Neuron activation value: {act_val}.\")\n",
    "        messages.add_content_image(image, detail=detail)\n",
    "\n",
    "    messages.add_content_text(question_text)\n",
    "    gt_ranks = []\n",
    "    names = []\n",
    "    for i, (image_index, class_name, act_val) in enumerate(zip(test_samples[IMAGE_INDEX], test_samples[GT_CLASS], test_samples[ACTIVATION_VALUE])):\n",
    "        name = \"ABCDEFGHIJKLMNOP\"[i] # there are some obvious limitations to this line lol\n",
    "        names.append(name)\n",
    "        class_name =imagenet_class_names[class_name]\n",
    "        act_val = round(act_val, 2)\n",
    "        image = imagenet_data[image_index][0].cpu().numpy()\n",
    "        image = np.uint8(255*np.transpose(image, (1, 2, 0)))\n",
    "        \n",
    "        rank = test_samples_indices.index(random_test_indices[i])+1\n",
    "        gt_ranks.append(rank) \n",
    "        messages.add_content_text(f\"Name: {name}. Class name: {class_name}\")\n",
    "        messages.add_comment(f\"Rank: {rank}. Class name: {class_name}. Neuron activation value: {act_val}.\")\n",
    "\n",
    "        messages.add_content_image(image, detail=detail)\n",
    "    messages.add_content_text(final_text)\n",
    "\n",
    "\n",
    "    \n",
    "    output = messages.run_gpt()\n",
    "\n",
    "    #TODO figure out how to run in json mode! (does gpt-v support?)\n",
    "    try:\n",
    "        try:\n",
    "            output = json.loads(output)\n",
    "        except:\n",
    "            # remove chatgpts format ```json\\n...\\b```\n",
    "            output = output.replace(\"```json\",\"\").replace(\"```\",\"\").strip()\n",
    "            output = json.loads(output)\n",
    "        \n",
    "        pred_rank_names = output[\"ranking\"]\n",
    "        pred_ranks = [pred_rank_names.index(name) + 1 for name in names]\n",
    "    except:\n",
    "        pred_ranks = None \n",
    "\n",
    "        messages.add_comment(f\"Model messed up output format!\")\n",
    "\n",
    "\n",
    "    if pred_ranks is not None:\n",
    "        score = pearsonr(gt_ranks, pred_ranks)\n",
    "        r_val, p_val = score[0], score[1]\n",
    "\n",
    "        messages.add_comment(f\"\\nModel output: {pred_ranks}\\nGround truth: {gt_ranks}\\nFINAL AUTOINTERPRETABILITY SCORE: {r_val}\\np val: {p_val}\")\n",
    "        # hack comment to top\n",
    "        messages.human_readable_content = [messages.human_readable_content[-1]] + messages.human_readable_content[:-1]                                                                                                                             \n",
    "        r_values.append(r_val)\n",
    "        p_values.append(p_val)\n",
    "    messages.convert_to_pdf(os.path.join(gpt_output_folder, f\"{layer_num:02d}_{neuron_idx:04d}_auto_interp.pdf\"))\n",
    "   # messages.jupyter_notebook_display()\n",
    "\n",
    "    count = count + 1\n",
    "    if hacky_max is not None and count >= hacky_max:\n",
    "        break\n",
    "print(r_values)\n",
    "print(p_values)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966948ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d840948",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
