{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "OUTPUT_FOLDER = \"\" # output directory\n",
    "import os\n",
    "#os.makedirs(OUTPUT_FOLDER,exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH= \"\" # trainingscirp checkpoint path #TODO shouldn't be needed.\n",
    "IMAGENET_PATH = \"\" #'folder containing imagenet1k data organized as follows: https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description'\n",
    "\n",
    "SAE_PATH = \"\"# path to SAE folder, might look something like \"final_sae_group_wkcn_TinyCLIP-ViT-40M-32-Text-19M-LAION400M_blocks.{layer}.mlp.hook_post_16384\"\n",
    "AUTOENCODER_NAME = \"\" #name of the particular sae within group (all names will get printed below )\n",
    "#model specs TODO these should be infered from pretrained model checkpoint (if they aren't already)\n",
    "\n",
    "#Optional\n",
    "HUGGINGFACE_CACHE_DIR = None\n",
    "LAYERS =  9\n",
    "EXPANSION_FACTOR = 8\n",
    "D_IN = 2048\n",
    "MODEL_NAME = \"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\"\n",
    "CONTEXT_SIZE = 50 \n",
    "PATCH_SIZE = 32\n",
    "HOOKPOINT = \"blocks.{layer}.mlp.hook_post\"\n",
    "LEGACY_LOAD= False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval constants\n",
    "\n",
    "EVAL_MAX = 10_000 \n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae.main import setup, ImageNetValidationDataset\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import einops\n",
    "from transformers import CLIPProcessor\n",
    "from vit_prisma.utils.data_utils.imagenet_dict import IMAGENET_DICT\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = load_dataset('Prisma-Multimodal/segmented-imagenet1k-subset', cache_dir =HUGGINGFACE_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, dataset, patch_size=32, width=224, height=224, return_label = True):\n",
    "        \"\"\"\n",
    "        dataset: A list of dictionaries, each dictionary corresponds to an image and its details\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "        self.transform =  torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.Resize((224, 224)),\n",
    "                        torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        #TODO for clip only \n",
    "                        torchvision.transforms.Normalize(mean=clip_processor.image_processor.image_mean,\n",
    "                        std=clip_processor.image_processor.image_std), ])\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.return_label = return_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = self.transform(item['image'])\n",
    "        if self.return_label:\n",
    "            masks = item['masks']\n",
    "            labels = item['labels']  # Assuming labels are aligned with masks\n",
    "            \n",
    "            # Calculate the size of the reduced mask\n",
    "            num_patches = self.width // self.patch_size\n",
    "            label_array = [[[] for _ in range(num_patches)] for _ in range(num_patches)]\n",
    "            \n",
    "            for mask, label in zip(masks, labels):\n",
    "                # Resize and reduce the mask\n",
    "                mask = mask.resize((self.width, self.height))\n",
    "                mask_array = np.array(mask) > 0\n",
    "                reduced_mask = self.reduce_mask(mask_array)\n",
    "                \n",
    "                # Populate the label array based on the reduced mask\n",
    "                for i in range(num_patches):\n",
    "                    for j in range(num_patches):\n",
    "                        if reduced_mask[i, j]:\n",
    "                            label_array[i][j].append(label)\n",
    "            \n",
    "            # Convert label_array to a format suitable for tensor operations, if necessary\n",
    "            # For now, it's a list of lists of lists, which can be used directly in Python\n",
    "            \n",
    "            return image, label_array, idx\n",
    "        else:\n",
    "            return image, idx \n",
    "    \n",
    "\n",
    "    def reduce_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Reduce the mask size by dividing it into patches and checking if there's at least\n",
    "        one True value within each patch.\n",
    "        \"\"\"\n",
    "        # Calculate new height and width\n",
    "        new_h = mask.shape[0] // self.patch_size\n",
    "        new_w = mask.shape[1] // self.patch_size\n",
    "        \n",
    "        reduced_mask = np.zeros((new_h, new_w), dtype=bool)\n",
    "        \n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                patch = mask[i*self.patch_size:(i+1)*self.patch_size, j*self.patch_size:(j+1)*self.patch_size]\n",
    "                reduced_mask[i, j] = np.any(patch)  # Set to True if any value in the patch is True\n",
    "        \n",
    "        return reduced_mask\n",
    "\n",
    "def collate_fn(data):\n",
    "    images = torch.stack([d[0] for d in data])\n",
    "    ids = [d[1] for d in data]\n",
    "    return images, ids\n",
    "\n",
    "patch_label_dataset = PatchDataset(default_dataset['train'], return_label=False)\n",
    "patch_label_dataset_with_label = PatchDataset(default_dataset['train'], return_label=True)\n",
    "im, idx = patch_label_dataset[0]\n",
    "print(im.shape)\n",
    "print(idx)\n",
    "im, l, idx = patch_label_dataset_with_label[0]\n",
    "print(im.shape)\n",
    "print(l)\n",
    "print(idx)\n",
    "data_loader = DataLoader(patch_label_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "im, idx = next(iter(data_loader))\n",
    "print(im.shape)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# setup model\n",
    "cfg ,model, activations_loader, sae_group = setup(checkpoint_path=CHECKPOINT_PATH, \n",
    "                                                  imagenet_path=IMAGENET_PATH ,\n",
    "                                                    pretrained_path=SAE_PATH, layers= LAYERS, expansion_factor=EXPANSION_FACTOR,\n",
    "                                                    model_name=MODEL_NAME, context_size=CONTEXT_SIZE, d_in=D_IN, hook_point=HOOKPOINT, legacy_load=LEGACY_LOAD)\n",
    "model = model.to(device)\n",
    "for i, (name, sae) in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Name: {name} Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = sae_group.autoencoders[AUTOENCODER_NAME]\n",
    "sparse_autoencoder = sparse_autoencoder.to(device)\n",
    "layer_num = sparse_autoencoder.cfg.hook_point_layer\n",
    "print(f\"Chosen layer {layer_num} hook point {sparse_autoencoder.cfg.hook_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper functions\n",
    "update_layout_set = {\"xaxis_range\", \"yaxis_range\", \"hovermode\", \"xaxis_title\", \"yaxis_title\", \"colorbar\", \"colorscale\", \"coloraxis\", \"title_x\", \"bargap\", \"bargroupgap\", \"xaxis_tickformat\", \"yaxis_tickformat\", \"title_y\", \"legend_title_text\", \"xaxis_showgrid\", \"xaxis_gridwidth\", \"xaxis_gridcolor\", \"yaxis_showgrid\", \"yaxis_gridwidth\", \"yaxis_gridcolor\", \"showlegend\", \"xaxis_tickmode\", \"yaxis_tickmode\", \"margin\", \"xaxis_visible\", \"yaxis_visible\", \"bargap\", \"bargroupgap\", \"coloraxis_showscale\"}\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Helper function to convert a tensor to a numpy array. Also works on lists, tuples, and numpy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return tensor\n",
    "    elif isinstance(tensor, (list, tuple)):\n",
    "        array = np.array(tensor)\n",
    "        return array\n",
    "    elif isinstance(tensor, (torch.Tensor, torch.nn.parameter.Parameter)):\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    elif isinstance(tensor, (int, float, bool, str)):\n",
    "        return np.array(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Input to to_numpy has invalid type: {type(tensor)}\")\n",
    "\n",
    "def hist(tensor, save_name, show=True, renderer=None, **kwargs):\n",
    "    '''\n",
    "    '''\n",
    "    kwargs_post = {k: v for k, v in kwargs.items() if k in update_layout_set}\n",
    "    kwargs_pre = {k: v for k, v in kwargs.items() if k not in update_layout_set}\n",
    "    if \"bargap\" not in kwargs_post:\n",
    "        kwargs_post[\"bargap\"] = 0.1\n",
    "    if \"margin\" in kwargs_post and isinstance(kwargs_post[\"margin\"], int):\n",
    "        kwargs_post[\"margin\"] = dict.fromkeys(list(\"tblr\"), kwargs_post[\"margin\"])\n",
    "\n",
    "    histogram_fig = px.histogram(x=to_numpy(tensor), **kwargs_pre)\n",
    "    histogram_fig.update_layout(**kwargs_post)\n",
    "\n",
    "    # Save the figure as a PNG file\n",
    "    histogram_fig.write_image(os.path.join(OUTPUT_FOLDER, f\"{save_name}.png\"))\n",
    "    if show:\n",
    "        px.histogram(x=to_numpy(tensor), **kwargs_pre).update_layout(**kwargs_post).show(renderer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_feature_probability(\n",
    "    images,\n",
    "    model,\n",
    "    sparse_autoencoder,\n",
    "):\n",
    "    '''\n",
    "    Returns the feature probabilities (i.e. fraction of time the feature is active) for each feature in the\n",
    "    autoencoder, averaged over all `batch * seq` tokens.\n",
    "    '''\n",
    "    _, cache = model.run_with_cache(images)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    class_acts = feature_acts[:, 0, :]\n",
    "    post_reshaped = einops.repeat(feature_acts, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "\n",
    "    return post_reshaped.mean(0), class_acts.mean(0)\n",
    "\n",
    "total_acts = None\n",
    "total_class_acts = None\n",
    "this_max = 500\n",
    "for batch_idx, (total_images, _) in tqdm(enumerate(data_loader), total=this_max//BATCH_SIZE):\n",
    "        total_images = total_images.to(device)\n",
    "        new, new_class = get_feature_probability(total_images, model, sparse_autoencoder)\n",
    "\n",
    "        if total_acts is None:\n",
    "             total_acts = new\n",
    "             total_class_acts = new_class \n",
    "        else:\n",
    "             total_acts = total_acts + new \n",
    "             total_class_acts = total_class_acts + new_class\n",
    "\n",
    "\n",
    "        if batch_idx*BATCH_SIZE >= this_max:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_probability = total_acts/(this_max//BATCH_SIZE)\n",
    "\n",
    "log_freq = (feature_probability + 1e-10).log10()\n",
    "\n",
    "feature_probability_class = total_class_acts/(this_max//BATCH_SIZE)\n",
    "\n",
    "log_freq_class = (feature_probability_class + 1e-10).log10()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_probability)\n",
    "def visualize_sparsities(log_freq, conditions, condition_texts, name):\n",
    "    # Visualise sparsities for each instance\n",
    "    hist(\n",
    "        log_freq,\n",
    "        f\"{name}_frequency_histogram\",\n",
    "        show=True,\n",
    "        title=f\"{name} Log Frequency of Features\",\n",
    "        labels={\"x\": \"log<sub>10</sub>(freq)\"},\n",
    "        histnorm=\"percent\",\n",
    "        template=\"ggplot2\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    for condition, condition_text in zip(conditions, condition_texts):\n",
    "        percentage = (torch.count_nonzero(condition)/log_freq.shape[0]).item()*100\n",
    "        if percentage == 0:\n",
    "            continue\n",
    "        percentage = int(np.round(percentage))\n",
    "        rare_encoder_directions = sparse_autoencoder.W_enc[:, condition]\n",
    "        rare_encoder_directions_normalized = rare_encoder_directions / rare_encoder_directions.norm(dim=0, keepdim=True)\n",
    "\n",
    "        # Compute their pairwise cosine similarities & sample randomly from this N*N matrix of similarities\n",
    "        cos_sims_rare = (rare_encoder_directions_normalized.T @ rare_encoder_directions_normalized).flatten()\n",
    "        cos_sims_rare_random_sample = cos_sims_rare[torch.randint(0, cos_sims_rare.shape[0], (10000,))]\n",
    "\n",
    "        # Plot results\n",
    "        hist(\n",
    "            cos_sims_rare_random_sample,\n",
    "            f\"{name}_low_prop_similarity_{condition_text}\",\n",
    "            show=True,\n",
    "            marginal=\"box\",\n",
    "            title=f\"{name} Cosine similarities of random {condition_text} encoder directions with each other ({percentage}% of features)\",\n",
    "            labels={\"x\": \"Cosine sim\"},\n",
    "            histnorm=\"percent\",\n",
    "            template=\"ggplot2\",\n",
    "        )\n",
    "\n",
    "#TODO these conditions should be tuned to distribution of your data!\n",
    "conditions = [ torch.logical_and(log_freq < -3,log_freq > -4)]\n",
    "condition_texts = [  \"logfreq_[-4,-3]\"]\n",
    "visualize_sparsities(log_freq, conditions, condition_texts, \"TOTAL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get random features from different bins\n",
    "\n",
    "interesting_features_indices = []\n",
    "interesting_features_values = []\n",
    "interesting_features_category = []\n",
    "number_features_per = 25\n",
    "for condition, condition_text in zip(conditions, [f\"TOTAL_{c}\" for c in condition_texts]):\n",
    "    \n",
    "\n",
    "    potential_indices = torch.nonzero(condition, as_tuple=True)[0]\n",
    "\n",
    "    # Shuffle these indices and select a subset\n",
    "    sampled_indices = potential_indices[torch.randperm(len(potential_indices))[:number_features_per]]\n",
    "\n",
    "    values = log_freq[sampled_indices]\n",
    "\n",
    "    interesting_features_indices = interesting_features_indices + sampled_indices.tolist()\n",
    "    interesting_features_values = interesting_features_values + values.tolist()\n",
    "\n",
    "    interesting_features_category = interesting_features_category + [f\"{condition_text}\"]*len(sampled_indices)\n",
    "\n",
    "\n",
    "# for v,i, c in zip(interesting_features_indices, interesting_features_values, interesting_features_category):\n",
    "#     print(c, v,i)\n",
    "\n",
    "print(set(interesting_features_category))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.no_grad()\n",
    "def highest_activating_tokens(\n",
    "    images,\n",
    "    model,\n",
    "    sparse_autoencoder,\n",
    "    W_enc,\n",
    "    b_enc,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories,\n",
    "    k: int = 10,\n",
    "):\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "\n",
    "    # Get the post activations from the clean run\n",
    "    _, cache = model.run_with_cache(images)\n",
    "\n",
    "    inp = cache[sparse_autoencoder.cfg.hook_point]\n",
    "    b, seq_len, _ = inp.shape\n",
    "    post_reshaped = einops.rearrange( inp, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            W_enc,\n",
    "            \"... d_in, d_in n -> ... n\",\n",
    "        )\n",
    "    \n",
    "    acts = acts + b_enc\n",
    "    acts = torch.nn.functional.relu(acts)\n",
    "    #TODO clean up\n",
    "    unshape = einops.rearrange(acts, \"(batch seq) d_in -> batch seq d_in\", batch=b, seq=seq_len)\n",
    "    cls_acts = unshape[:,0,:]\n",
    "    per_image_acts = unshape.mean(1)\n",
    "\n",
    "\n",
    "    to_return = {} \n",
    "    #TODO this is a bad way to do it.\n",
    "    for i, (feature_id, feature_cat) in enumerate(zip(feature_ids, feature_categories)):\n",
    "        if \"CLS_\" in feature_cat:\n",
    "            top_acts_values, top_acts_indices = cls_acts[:,i].topk(k)\n",
    "\n",
    "            to_return[feature_id]  = (top_acts_indices, top_acts_values)\n",
    "        else:\n",
    "            top_acts_values, top_acts_indices = per_image_acts[:,i].topk(min(k,per_image_acts[:,i].shape[0]))\n",
    "\n",
    "            to_return[feature_id]  = (top_acts_indices, top_acts_values)\n",
    "    return to_return, per_image_acts\n",
    "this_max = EVAL_MAX\n",
    "\n",
    "max_indices = {i:None for i in interesting_features_indices}\n",
    "max_values =  {i:None for i in interesting_features_indices}\n",
    "\n",
    "# for each feature, the total activation of a given image\n",
    "all_activations_per_image = torch.zeros( len(patch_label_dataset), (len(interesting_features_indices))).to(device)\n",
    "\n",
    "#print(all_activations_per_image.shape) \n",
    "b_enc = sparse_autoencoder.b_enc[interesting_features_indices]\n",
    "W_enc = sparse_autoencoder.W_enc[:, interesting_features_indices]\n",
    "for batch_idx, (total_images, total_indices) in tqdm(enumerate(data_loader), total=this_max//BATCH_SIZE):\n",
    "        total_images = total_images.to(device)\n",
    "        total_indices = torch.tensor(total_indices).to(device)\n",
    "        new_stuff, new_per_image_acts = highest_activating_tokens(total_images, model, sparse_autoencoder, W_enc, b_enc, interesting_features_indices, interesting_features_category, k=25)\n",
    "        all_activations_per_image[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE + new_per_image_acts.shape[0], :] = new_per_image_acts\n",
    "       # print(new_per_image_acts.shape)\n",
    "        for feature_id in interesting_features_indices:\n",
    "\n",
    "            new_indices, new_values = new_stuff[feature_id]\n",
    "            new_indices = total_indices[new_indices]\n",
    "            #  new_indices[:,0] = new_indices[:,0] + batch_idx*batch_size\n",
    "            \n",
    "            if max_indices[feature_id] is None:\n",
    "                max_indices[feature_id] = new_indices\n",
    "                max_values[feature_id] = new_values\n",
    "            else:\n",
    "                ABvals = torch.cat((max_values[feature_id], new_values))\n",
    "                ABinds = torch.cat((max_indices[feature_id], new_indices))\n",
    "                _, inds = torch.topk(ABvals, 25)\n",
    "                max_values[feature_id] = ABvals[inds]\n",
    "                max_indices[feature_id] = ABinds[inds]\n",
    "           # print(max_indices[feature_id].shape)\n",
    "\n",
    "        if batch_idx*BATCH_SIZE >= this_max:\n",
    "            break\n",
    "all_activations_per_image = all_activations_per_image.detach().cpu()\n",
    "top_per_feature = {i:(max_values[i].detach().cpu(), max_indices[i].detach().cpu()) for i in interesting_features_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.no_grad()\n",
    "def get_heatmap(\n",
    "          \n",
    "          image,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "): \n",
    "    image = image.to(device)\n",
    "    _, cache = model.run_with_cache(image.unsqueeze(0))\n",
    "\n",
    "    post_reshaped = einops.rearrange( cache[sparse_autoencoder.cfg.hook_point], \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"x d_in, d_in -> x\",\n",
    "        )\n",
    "    return acts \n",
    "     \n",
    "import pandas as pd\n",
    "\n",
    "num_rows = len(list(top_per_feature.keys()))*(25+25)*50\n",
    "\n",
    "# Define the column names and types\n",
    "columns = {\n",
    "    'patchIdx': np.int32,\n",
    "    'featureIdx': np.int32,\n",
    "    'imageIdx': np.int32,\n",
    "    'activationValue': np.float32,\n",
    "    'label': object,  # Strings are object type in pandas\n",
    "    'type': object,\n",
    "    'layerIdx': np.int32\n",
    "}\n",
    "\n",
    "# Create the DataFrame with preallocated data\n",
    "df = pd.DataFrame({\n",
    "    'patchIdx': np.zeros(num_rows, dtype=np.int32),\n",
    "    'featureIdx': np.zeros(num_rows, dtype=np.int32),\n",
    "    'imageIdx': np.zeros(num_rows, dtype=np.int32),\n",
    "    'activationValue': np.zeros(num_rows, dtype=np.float32),\n",
    "    'label': [''] * num_rows,  # Initialize with empty strings\n",
    "    'type': [''] * num_rows,   # Initialize with empty strings\n",
    "    'layerIdx': np.zeros(num_rows, dtype=np.int32)\n",
    "})\n",
    "\n",
    "# Print the updated table\n",
    "# Save the table to a Parquet file\n",
    "df_count = 0\n",
    "for feature_ids, cat, logfreq in tqdm(zip(top_per_feature.keys(), interesting_features_category, interesting_features_values), total=len(interesting_features_category)):\n",
    "  #  print(f\"looking at {feature_ids}, {cat}\")\n",
    "    _, max_inds = top_per_feature[feature_ids]\n",
    "    max_inds = [m.item() for m in max_inds]\n",
    "    if len(max_inds) != len(set(max_inds)):\n",
    "        print(\"SKIPPING\")\n",
    "        continue\n",
    "    images = []\n",
    "    gt_labels = []\n",
    "    is_random = []\n",
    "    bids = []\n",
    "    for bid in max_inds:\n",
    "        image, labels, image_ind = patch_label_dataset_with_label[bid]\n",
    "\n",
    "        assert image_ind == bid\n",
    "        images.append(image)\n",
    "\n",
    "        gt_labels.append(labels)\n",
    "        is_random.append(\"top\")\n",
    "        bids.append(bid)\n",
    "\n",
    "    for _ in range(len(max_inds)):\n",
    "        bid = np.random.randint(0, len(patch_label_dataset_with_label) )\n",
    "        image, labels, image_ind = patch_label_dataset_with_label[bid]\n",
    "\n",
    "        assert image_ind == bid\n",
    "        images.append(image)\n",
    "\n",
    "        gt_labels.append(labels)\n",
    "        is_random.append(\"random\")\n",
    "        bids.append(bid)\n",
    "\n",
    "\n",
    "    for i, (all_label, bid,img, typestr) in enumerate(zip(gt_labels,bids,images, is_random )):\n",
    "\n",
    "        # TODO create entries \n",
    " \n",
    "\n",
    "        # image = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "        # # Display the image\n",
    "        # plt.imshow(image)\n",
    "        # plt.axis('off')  # Turn off axis labels\n",
    "        # plt.show()\n",
    "        heatmap = get_heatmap(img,model,sparse_autoencoder, feature_ids )\n",
    "        featureIdx = feature_ids\n",
    "        imageIdx = bid \n",
    "        layerIdx = LAYERS\n",
    "        \n",
    "    # ('patchIdx', pa.int32()),\n",
    "    # ('featureIdx', pa.int32()),\n",
    "    # ('imageIdx', pa.int32()),\n",
    "    # ('activationValue', pa.float32()),\n",
    "    # ('label', pa.string()),\n",
    "    # ('type', pa.string()),\n",
    "    # (\"layerIdx\", pa.int32())\n",
    "        for patchIdx in range(heatmap.shape[0]):\n",
    "            activationValue = heatmap[patchIdx].item()\n",
    "            if patchIdx == 0:\n",
    "                label = \"\"\n",
    "            else:\n",
    "                pi = (patchIdx-1)//7 # TODO general\n",
    "                pj = (patchIdx-1)%7\n",
    "                labels = list(set(all_label[pi][pj]))\n",
    "                label = \", \".join(labels)\n",
    "\n",
    "            row = [patchIdx, featureIdx, imageIdx, activationValue, label, typestr, layerIdx]\n",
    "            new_data = {\n",
    "                'patchIdx': patchIdx,\n",
    "                'featureIdx': featureIdx,\n",
    "                'imageIdx': imageIdx,\n",
    "                'activationValue':activationValue,\n",
    "                'label': label,\n",
    "                'type': typestr,\n",
    "                'layerIdx': layerIdx\n",
    "            }\n",
    "\n",
    "            df.loc[df_count] = new_data\n",
    "            df_count += 1\n",
    "\n",
    "    \n",
    "df.to_parquet(os.path.join(OUTPUT_FOLDER,'example.parquet'))\n",
    "\n",
    "   # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_activations_per_image, os.path.join(OUTPUT_FOLDER, \"all_activations.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(all_activations_per_image)\n",
    "\n",
    "for feature_id, (mvalues, mindices) in top_per_feature.items():\n",
    "    acts, act_indices = torch.sort(all_activations_per_image[:,0], descending=True)\n",
    "    print(mvalues.tolist())\n",
    "    print(mindices.tolist())\n",
    "\n",
    "    print(act_indices[0:25].tolist())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(OUTPUT_FOLDER,'example.parquet'))\n",
    "31250\n",
    "print(os.path.join(OUTPUT_FOLDER,'example.parquet'))\n",
    "print(len(list(top_per_feature.keys()))*(25+25)*50)\n",
    "# Define the number of rows to display\n",
    "n = 10\n",
    "print(df_count)\n",
    "# Display the first n rows of the DataFrame\n",
    "print(df.head(n))\n",
    "print(df.tail(n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
