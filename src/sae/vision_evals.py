from functools import partial
from typing import Any, cast

import pandas as pd
import torch
import wandb
from tqdm import tqdm
from transformer_lens import HookedTransformer
from transformer_lens.utils import get_act_name

from sae.vision_activations_store import VisionActivationsStore
import torch.nn.functional as F
# Taken from SAELens but adapted slightly for vision. #TODO adapt for CLIP model!





def zero_ablate_hook(activations: torch.Tensor, hook: Any):
    activations = torch.zeros_like(activations)
    return activations


def kl_divergence_attention(y_true: torch.Tensor, y_pred: torch.Tensor):
    # Compute log probabilities for KL divergence
    log_y_true = torch.log2(y_true + 1e-10)
    log_y_pred = torch.log2(y_pred + 1e-10)

    return y_true * (log_y_true - log_y_pred)

@torch.no_grad()
def run_evals_vision(
    sparse_autoencoder,
    activation_store: VisionActivationsStore,
    model: HookedTransformer,
    n_training_steps: int,
):
    """
    Runs evaluations on a vision model using sparse autoencoders and logs metrics.

    This function is adapted from language model evaluation for vision models.
    It evaluates the model by reconstructing activations and logging various metrics to Weights & Biases.

    Args:
        sparse_autoencoder: The sparse autoencoder to be evaluated.
        activation_store (VisionActivationsStore): Store for vision model activations.
        model (HookedTransformer): The vision model to be evaluated.
        n_training_steps (int): The current number of training steps.

    """
    
    hook_point = sparse_autoencoder.cfg.hook_name
    hook_point_head_index = sparse_autoencoder.cfg.hook_head_index
    ### Evals
    eval_tokens = activation_store.get_val_batch_tokens()[0]

    #TODO this was set up with classifier vit in mind but hasn't been modified for clip
    # Get Reconstruction Score
    # losses_df = recons_loss_batched(
    #     sparse_autoencoder,
    #     model,
    #     activation_store,
    #     n_batches=10,
    # )


    # recons_score = losses_df["score"].mean()
    # ntp_loss = losses_df["loss"].mean()
    # recons_loss = losses_df["recons_loss"].mean()
    # zero_abl_loss = losses_df["zero_abl_loss"].mean()

    # get cache
    _, cache = model.run_with_cache(
        eval_tokens,
        names_filter=[hook_point],
    )

    # get act
    if hook_point_head_index is not None:
        original_act = cache[hook_point][
            :, :, hook_point_head_index
        ]
    else:
        original_act = cache[hook_point]

    sae_out = sparse_autoencoder(original_act)
    patterns_original = (
        cache[hook_point][:, hook_point_head_index]
        .detach()
        .cpu()
    )
    del cache

    if "cuda" in str(model.cfg.device):
        torch.cuda.empty_cache()

    l2_norm_in = torch.norm(original_act, dim=-1)
    l2_norm_out = torch.norm(sae_out, dim=-1)
    l2_norm_ratio = l2_norm_out / l2_norm_in

    wandb.log(
        {
            # l2 norms
            f"metrics/l2_norm": l2_norm_out.mean().item(),
            f"metrics/l2_ratio": l2_norm_ratio.mean().item(),
            # CE Loss
            # f"metrics/CE_loss_score{suffix}": recons_score,
            # f"metrics/ce_loss_without_sae{suffix}": ntp_loss,
            # f"metrics/ce_loss_with_sae{suffix}": recons_loss,
            # f"metrics/ce_loss_with_ablation{suffix}": zero_abl_loss,
        },
        step=n_training_steps,
    )


    def standard_replacement_hook(activations: torch.Tensor, hook: Any):
        """
        Replaces the activations in a model layer with those generated by the sparse autoencoder.

        This hook function is used when there is no specific head index specified. It processes the entire set of activations through the sparse autoencoder.

        Args:
            activations (torch.Tensor): The original activations from the model layer.
            hook (Any): The hook object (not used in this function).

        Returns:
            torch.Tensor: The activations after being processed by the sparse autoencoder.
        """

        activations = sparse_autoencoder.forward(activations).to(activations.dtype)
        return activations

    def head_replacement_hook(activations: torch.Tensor, hook: Any):
        """
        Replaces the activations in a specific head of a model layer with those generated by the sparse autoencoder.

        This hook function is used when a specific head index is specified. It processes the activations of the specified head through the sparse autoencoder and replaces only those activations in the original tensor.

        Args:
            activations (torch.Tensor): The original activations from the model layer.
            hook (Any): The hook object (not used in this function).

        Returns:
            torch.Tensor: The activations after being processed by the sparse autoencoder for the specified head.
        """
                
        new_actions = sparse_autoencoder.forward(activations[:, :, hook_point_head_index]).to(
            activations.dtype
        )
        activations[:, :, hook_point_head_index] = new_actions
        return activations

    replacement_hook = (
        standard_replacement_hook if hook_point_head_index is None else head_replacement_hook
    )

    # get attn when using reconstructed activations
    with model.hooks(fwd_hooks=[(hook_point, partial(replacement_hook))]):
        _, new_cache = model.run_with_cache(
            eval_tokens, names_filter=[hook_point]
        )
        patterns_reconstructed = (
            new_cache[hook_point][
                :, hook_point_head_index
            ]
            .detach()
            .cpu()
        )
        del new_cache

    # get attn when using reconstructed activations
    with model.hooks(fwd_hooks=[(hook_point, partial(zero_ablate_hook))]):
        _, zero_ablation_cache = model.run_with_cache(
            eval_tokens, names_filter=[hook_point]
        )
        patterns_ablation = (
            zero_ablation_cache[hook_point][
                :, hook_point_head_index
            ]
            .detach()
            .cpu()
        )
        del zero_ablation_cache

    if sparse_autoencoder.cfg.hook_head_index:
        kl_result_reconstructed = kl_divergence_attention(
            patterns_original, patterns_reconstructed
        )
        kl_result_reconstructed = kl_result_reconstructed.sum(dim=-1).numpy()

        kl_result_ablation = kl_divergence_attention(
            patterns_original, patterns_ablation
        )
        kl_result_ablation = kl_result_ablation.sum(dim=-1).numpy()

        if wandb.run is not None:
            wandb.log(
                {
                    f"metrics/kldiv_reconstructed": kl_result_reconstructed.mean().item(),
                    f"metrics/kldiv_ablation": kl_result_ablation.mean().item(),
                },
                step=n_training_steps,
            )

def recons_loss_batched(
    sparse_autoencoder,
    model: HookedTransformer,
    activation_store: VisionActivationsStore,
    n_batches: int = 100,
):
    """
    Computes reconstruction loss metrics for a given number of batches using a sparse autoencoder and vision model.

    Args:
        sparse_autoencoder: The sparse autoencoder used for reconstruction.
        model (HookedTransformer): The vision model from which activations are obtained.
        activation_store (VisionActivationsStore): Store for vision model activations.
        n_batches (int, optional): The number of batches to compute the reconstruction loss over. Default is 100.

    Returns:
        pd.DataFrame: DataFrame containing the computed losses with columns ["score", "loss", "recons_loss", "zero_abl_loss"].
    """

    losses = []
    for _ in tqdm(range(n_batches)):
        batch_tokens, labels = activation_store.get_val_batch_tokens()
        
        score, loss, recons_loss, zero_abl_loss = get_recons_loss(
            sparse_autoencoder, model, batch_tokens, labels,
        )
        losses.append(
            (
                score.mean().item(),
                loss.mean().item(),
                recons_loss.mean().item(),
                zero_abl_loss.mean().item(),
            )
        )

    losses = pd.DataFrame(
        losses, columns=cast(Any, ["score", "loss", "recons_loss", "zero_abl_loss"])
    )

    return losses


@torch.no_grad()
def get_recons_loss(
    sparse_autoencoder,
    model: HookedTransformer,
    batch_tokens: torch.Tensor,
    labels:torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Computes reconstruction loss and related metrics for a given batch of tokens and labels using a sparse autoencoder and vision model.

    Args:
        sparse_autoencoder: The sparse autoencoder used for reconstruction.
        model (HookedTransformer): The vision model from which activations are obtained.
        batch_tokens (torch.Tensor): A batch of tokenized inputs for the vision model.
        labels (torch.Tensor): Ground truth labels for the inputs.

    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:
            - score (torch.Tensor): The reconstruction score.
            - loss (torch.Tensor): The cross-entropy loss of the original model outputs.
            - recons_loss (torch.Tensor): The cross-entropy loss of the reconstructed model outputs.
            - zero_abl_loss (torch.Tensor): The cross-entropy loss after zero ablation.

    """


    hook_point = sparse_autoencoder.cfg.hook_point
    class_logits = model(batch_tokens)
    loss = F.cross_entropy(class_logits, labels)



    head_index = sparse_autoencoder.cfg.hook_point_head_index

    def standard_replacement_hook(activations: torch.Tensor, hook: Any):
        """
        Replaces the activations in a model layer with those generated by the sparse autoencoder.

        This hook function is used when there is no specific head index specified. It processes the entire set of activations through the sparse autoencoder.

        Args:
            activations (torch.Tensor): The original activations from the model layer.
            hook (Any): The hook object (not used in this function).

        Returns:
            torch.Tensor: The activations after being processed by the sparse autoencoder.
        """

        activations = sparse_autoencoder.forward(activations).to(activations.dtype)
        return activations

    def head_replacement_hook(activations: torch.Tensor, hook: Any):
        """
        Replaces the activations in a specific head of a model layer with those generated by the sparse autoencoder.

        This hook function is used when a specific head index is specified. It processes the activations of the specified head through the sparse autoencoder and replaces only those activations in the original tensor.

        Args:
            activations (torch.Tensor): The original activations from the model layer.
            hook (Any): The hook object (not used in this function).

        Returns:
            torch.Tensor: The activations after being processed by the sparse autoencoder for the specified head.
        """

        new_activations = sparse_autoencoder.forward(activations[:, :, head_index]).to(activations.dtype)
        activations[:, :, head_index] = new_activations
        return activations

    replacement_hook = (
        standard_replacement_hook if head_index is None else head_replacement_hook
    )
    recons_class_logits = model.run_with_hooks(
        batch_tokens,
        fwd_hooks=[(hook_point, partial(replacement_hook))],
    )
    recons_loss = F.cross_entropy(recons_class_logits, labels)

    zero_abl_class_logits = model.run_with_hooks(
        batch_tokens, fwd_hooks=[(hook_point, zero_ablate_hook)]
    )
    zero_abl_loss = F.cross_entropy(zero_abl_class_logits, labels)

    score = (zero_abl_loss - recons_loss) / (zero_abl_loss - loss)

    return score, loss, recons_loss, zero_abl_loss