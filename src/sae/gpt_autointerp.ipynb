{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths and so on\n",
    "PARQUET_INPUT = r\"\"\n",
    "ACTIVATIONS_INPUT =r\"\"\n",
    "OUTPUT_FOLDER = r\"\"\n",
    "\n",
    "#optional\n",
    "HUGGINGFACE_CACHE_DIR = None \n",
    "\n",
    "# parquet column names\n",
    "PATCH_IDX = \"patchIdx\"\n",
    "FEATURE_IDX = \"featureIdx\"\n",
    "IMAGE_IDX = \"imageIdx\"\n",
    "ACTIVATION_VALUE = \"activationValue\"\n",
    "LABEL = \"label\"\n",
    "TYPE = \"type\"\n",
    "LAYER_IDX = \"layerIdx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import torchvision \n",
    "import matplotlib.pyplot as plt\n",
    "from sae.basic_vision_api_call import UserMessage, ImageChatHistory, call_model\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os \n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# assuming  OPENAI_API_KEY and OPENAI_ORGANIZATION_ID is set in .env can also manually set it in env or pass it into call_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(PARQUET_INPUT)\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = load_dataset('Prisma-Multimodal/segmented-imagenet1k-subset', cache_dir=HUGGINGFACE_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, dataset, patch_size=32, width=224, height=224, return_label = True):\n",
    "        \"\"\"\n",
    "        dataset: A list of dictionaries, each dictionary corresponds to an image and its details\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform =  torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.Resize((224, 224)),\n",
    "                        torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Lambda(lambda img: img.permute(1, 2, 0))\n",
    "                        ])\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.return_label = return_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = self.transform(item['image'])\n",
    "        if self.return_label:\n",
    "            masks = item['masks']\n",
    "            labels = item['labels']  # Assuming labels are aligned with masks\n",
    "            \n",
    "            # Calculate the size of the reduced mask\n",
    "            num_patches = self.width // self.patch_size\n",
    "            label_array = [[[] for _ in range(num_patches)] for _ in range(num_patches)]\n",
    "            \n",
    "            for mask, label in zip(masks, labels):\n",
    "                # Resize and reduce the mask\n",
    "                mask = mask.resize((self.width, self.height))\n",
    "                mask_array = np.array(mask) > 0\n",
    "                reduced_mask = self.reduce_mask(mask_array)\n",
    "                \n",
    "                # Populate the label array based on the reduced mask\n",
    "                for i in range(num_patches):\n",
    "                    for j in range(num_patches):\n",
    "                        if reduced_mask[i, j]:\n",
    "                            label_array[i][j].append(label)\n",
    "            \n",
    "            # Convert label_array to a format suitable for tensor operations, if necessary\n",
    "            # For now, it's a list of lists of lists, which can be used directly in Python\n",
    "            \n",
    "            return image, label_array, idx\n",
    "        else:\n",
    "            return image, idx \n",
    "    \n",
    "\n",
    "    def reduce_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Reduce the mask size by dividing it into patches and checking if there's at least\n",
    "        one True value within each patch.\n",
    "        \"\"\"\n",
    "        # Calculate new height and width\n",
    "        new_h = mask.shape[0] // self.patch_size\n",
    "        new_w = mask.shape[1] // self.patch_size\n",
    "        \n",
    "        reduced_mask = np.zeros((new_h, new_w), dtype=bool)\n",
    "        \n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                patch = mask[i*self.patch_size:(i+1)*self.patch_size, j*self.patch_size:(j+1)*self.patch_size]\n",
    "                reduced_mask[i, j] = np.any(patch)  # Set to True if any value in the patch is True\n",
    "        \n",
    "        return reduced_mask\n",
    "\n",
    "\n",
    "\n",
    "patch_label_dataset = PatchDataset(default_dataset['train'], return_label=False)\n",
    "im, idx = patch_label_dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# for a given feature (and 'type'), find all images and heatmaps associated with it. \n",
    "def get_overlays_and_images(feature_id, type_label=\"top\", num_patches_w=7, low=0.25, high=0.75, display=False, return_indices=False):\n",
    "    filtered_df = df[df[FEATURE_IDX] == feature_id]\n",
    "    \n",
    "    # Group by both imageIdx and type\n",
    "    grouped = filtered_df.groupby([IMAGE_IDX, TYPE])\n",
    "    \n",
    "    images = []\n",
    "    overlays = []\n",
    "    indices = []\n",
    "    for (image_idx, type_), group in grouped:\n",
    "        if type_!=type_label:\n",
    "            continue\n",
    "        activations = group[ACTIVATION_VALUE].values\n",
    "        # get heatmap\n",
    "        heatmap = activations[1:].reshape((num_patches_w,num_patches_w))\n",
    "        image, _ = patch_label_dataset[int(image_idx)]\n",
    "        indices.append(int(image_idx))\n",
    "        image = image.detach().cpu().numpy()\n",
    "        # normalize\n",
    "        heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "        # clip\n",
    "        heatmap[np.logical_and(0.1 <= heatmap, heatmap <= low)] = 0.1\n",
    "        heatmap[heatmap >= high] = 1\n",
    "        # upscale\n",
    "        heatmap = np.repeat(np.repeat(heatmap, 224//num_patches_w, axis=0), 224//num_patches_w, axis=1)\n",
    "        #create 3rd dim\n",
    "        heatmap = np.stack([heatmap]*3, axis=-1)\n",
    "        overlay = image*heatmap\n",
    "\n",
    "        if display:\n",
    "            plt.imshow(overlay)\n",
    "            plt.show()\n",
    "        images.append(np.uint8(255*image))\n",
    "        overlays.append(np.uint8(255*overlay))\n",
    "    if return_indices:\n",
    "        return indices\n",
    "    return overlays, images\n",
    "\n",
    "\n",
    "_,_ = get_overlays_and_images(df[FEATURE_IDX].iloc[0], display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some hacky code to generate a pdf \n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.platypus import Image as ReportLabImage\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from PIL import Image \n",
    "import io\n",
    "\n",
    "from reportlab.platypus import PageBreak\n",
    "\n",
    "\n",
    "\n",
    "def resize_image(image, max_width, max_height):\n",
    "    # Calculate the new dimensions maintaining the aspect ratio\n",
    "    width_percent = max_width / float(image.size[0])\n",
    "    height_percent = max_height / float(image.size[1])\n",
    "    aspect_ratio = min(width_percent, height_percent)\n",
    "\n",
    "    # New dimensions\n",
    "    width = int((float(image.size[0]) * float(aspect_ratio)))\n",
    "    height = int((float(image.size[1]) * float(aspect_ratio)))\n",
    "\n",
    "    return image.resize((width, height), Image.ANTIALIAS)\n",
    "def create_pdf(content_list, file_name='output.pdf'):\n",
    "    doc = SimpleDocTemplate(file_name, pagesize=letter)\n",
    "    elements = []\n",
    "    style_sheet = getSampleStyleSheet()\n",
    "\n",
    "    #TODO figure out how to do this better.. \n",
    "    max_image_width = 456#letter[0] - 50\n",
    "    max_image_height = 636#letter[1] - 50\n",
    "    for item in content_list:\n",
    "        if isinstance(item, str):\n",
    "            if item == \"<pagebreak>\":\n",
    "                elements.append(PageBreak())\n",
    "                continue\n",
    "            # Add text\n",
    "            item = item.replace('\\n', '<br/>')\n",
    "\n",
    "            elements.append(Paragraph(item, style_sheet['BodyText']))\n",
    "            elements.append(Spacer(1, 12))  # Add space after paragraph\n",
    "\n",
    "        elif isinstance(item, np.ndarray):\n",
    "            # Convert numpy array to list of lists and create a table\n",
    "          #  item = item[:,:,0:3]\n",
    "            image = Image.fromarray(item.astype('uint8'), 'RGB')\n",
    "            if image.size[0] > max_image_width or image.size[1] > max_image_height:\n",
    "                image = resize_image(image, max_image_width, max_image_height)\n",
    "            image_buffer = io.BytesIO()\n",
    "            image.save(image_buffer, format='PNG')\n",
    "            image_buffer.seek(0)\n",
    "            img = ReportLabImage(image_buffer)\n",
    "            elements.append(img)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Content list must contain only strings and numpy arrays.\")\n",
    "\n",
    "    doc.build(elements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = df[FEATURE_IDX].unique()\n",
    "\n",
    "print(len(all_features))\n",
    "from io import BytesIO\n",
    "\n",
    "def plot_grid_of_arrays(arrays, show=True):\n",
    "    n = len(arrays)\n",
    "    grid_size_w = int(np.ceil(np.sqrt(n)))\n",
    "    grid_size_h = int(np.ceil(n/grid_size_w))\n",
    "    _, axes = plt.subplots(grid_size_h, grid_size_w, figsize=(grid_size_w*2, grid_size_h*2))\n",
    "    \n",
    "    for ax, array in zip(axes.flat, arrays):\n",
    "        ax.imshow(array, cmap='gray')  # Assuming the arrays are 2D grayscale images\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for ax in axes.flat[n:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', transparent=False)\n",
    "    buf.seek(0)\n",
    "\n",
    "    image = Image.open(buf)\n",
    "    image_rgb = image.convert('RGB')  # Convert to RGB\n",
    "    image_np = np.array(image_rgb)\n",
    "\n",
    "    buf.close()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_MESSAGE = \"The following are images of a TinyCLIP neuron's activations. We want to determine what the neuron is responding too. Areas of the image that are LESS relevant are darker. \\\n",
    "      Focus on the visible areas. Notice small details and patterns across the images. Respond in a concise, descriptive phrase exactly what it's selecting for, and \\\n",
    "            try not to use words neuron, activation, or image, or focus/selective/etc, because that will be redundant for our purposes. \\\n",
    "            Keep your words simple but be pretty specific and granular. For example, instead of saying 'face,' note the \\\n",
    "              specific parts of a face, like ears, neck, etc.\"\n",
    "num_images = 10\n",
    "num_features = 25 # len(all_features)\n",
    "pdf_content = []\n",
    "display_images = []\n",
    "descriptions = []\n",
    "final_feature_ids = []\n",
    "for feature_id in all_features[0:num_features]:\n",
    "    top_overlays, top_images = get_overlays_and_images(feature_id)\n",
    "\n",
    "    chat_history = ImageChatHistory()\n",
    "\n",
    "    chat_history.add_system_msg(SYSTEM_MESSAGE)\n",
    "\n",
    "\n",
    "    user_message_with_images = UserMessage()\n",
    "\n",
    "    user_message_with_images.add_text(\"Here are the images:\")\n",
    "\n",
    "    for i in range(num_images):\n",
    "        user_message_with_images.add_img_array(top_overlays[i])\n",
    "\n",
    "    chat_history.add_user_msg(user_message_with_images)\n",
    "\n",
    "\n",
    "    display_image = plot_grid_of_arrays(top_overlays[0:num_images])\n",
    "\n",
    "    description = call_model(chat_history, model=\"gpt-4o\")\n",
    "    strr = f\"----------------\\n{feature_id}\\n---------------\\n=====Model output:::\\n{description}\\n\"\n",
    "    print(strr)\n",
    "    display_images.append(display_image)\n",
    "    descriptions.append(description)\n",
    "    final_feature_ids.append(feature_id)\n",
    "    pdf_content.append(display_image)\n",
    "    pdf_content.append(strr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy hack to deal with openai errors \n",
    "to_delete = []\n",
    "for i in range(1,50,2):\n",
    "   # print(pdf_content[i])\n",
    "    feature_id = all_features[int((i-1)/2)]\n",
    "    if \"'message':\" in pdf_content[i]:\n",
    "        to_delete.append(i-1)\n",
    "        to_delete.append(i)\n",
    "\n",
    "to_delete = to_delete[::-1]\n",
    "\n",
    "for thing in to_delete:\n",
    "    del pdf_content[thing]\n",
    "    del display_images[thing]\n",
    "    del descriptions[thing]\n",
    "    del final_feature_ids[thing]\n",
    "       # pdf_content[i] = strr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "\n",
    "def get_image(image_idx):\n",
    "    \n",
    "    image, _ = patch_label_dataset[int(image_idx)]\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = np.uint8(255*image)\n",
    "    return  image\n",
    "\n",
    "import random\n",
    "import torch \n",
    "import json \n",
    "all_acts = torch.load(ACTIVATIONS_INPUT)\n",
    "pdf_content_ranking = []\n",
    "arrs = []\n",
    "texts = []\n",
    "scores = []\n",
    "assert len(final_feature_ids) == num_features, \"haven't set up to handle case where need to delete things above and I'm being lazy\"\n",
    "for i in range(len(final_feature_ids)):\n",
    "    acts = all_acts[:, i]\n",
    "\n",
    "    acts, act_indices = torch.sort(acts, descending=True)\n",
    "    p_vals = []\n",
    "    three_scores = []\n",
    "    total_score = 0\n",
    "\n",
    "    for _ in range(3):\n",
    "\n",
    "        #3 samples from top 10\n",
    "        size = 1\n",
    "        s = torch.randperm(10)[:3*size]\n",
    "        sample_inds = act_indices[0:10][s].tolist()\n",
    "        sample_acts = acts[0:10][s].tolist()\n",
    "        # 1 samples from top n to n + 10 for n = 10,20...\n",
    "  \n",
    "        for ii in range(12):\n",
    "            s = torch.randperm(10)[:1*size]\n",
    "            sample_inds = sample_inds + act_indices[10*(ii+1):10*(ii+1)+10][s].tolist()\n",
    "            sample_acts = sample_acts + acts[10*(ii+1):10*(ii+1)+10][s].tolist()\n",
    "\n",
    "        # 5 random \n",
    "        s =    torch.randint(0, acts.size(0), (5*size,))  \n",
    "        sample_inds = sample_inds + act_indices[s].tolist()\n",
    "        sample_acts = sample_acts + acts[s].tolist()\n",
    "\n",
    "\n",
    "        sorted_pairs = sorted(zip(sample_inds, sample_acts), key=lambda x: x[1],reverse=True)\n",
    "        sample_inds, sample_acts = zip(*sorted_pairs)\n",
    "        sample_inds = list(sample_inds)\n",
    "        sample_acts = list(sample_acts)\n",
    "        print(sample_inds, sample_acts)\n",
    "\n",
    "        images = [get_image(si) for si in sample_inds]\n",
    "        arr = plot_grid_of_arrays(images, show=True)\n",
    "\n",
    "        SYSTEM_MESSAGE = f\"The following are {len(images)} images of a TinyCLIP neuron's activations. The neuron was described as activating in response to the following stimulus: {descriptions[i]}.\\\n",
    "        Your job is to rank the images from highest activating to lowest. Each image has a name, A, B, C... Format your answer as a json dict with two keys. Under key 'info',\\\n",
    "            describe your thought process. Under key 'ranking' make a list of the names in order (example ['C', 'B', 'A']). Do not put comments in your json.\"\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        all_names =  \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"[0:len(images)]#obvious flaw if too many examples given haha\n",
    "\n",
    "        gt_names = [\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"[j] for j in range(len(images))] \n",
    "        gt_ranks = [j for j in range(len(images))]\n",
    "        random.shuffle(gt_names)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        chat_history = ImageChatHistory()\n",
    "        user_message_with_images = UserMessage()\n",
    "        chat_history.add_system_msg(SYSTEM_MESSAGE)\n",
    "        for k in range(len(images)):\n",
    "            name = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"[k]\n",
    "\n",
    "            cur_img_ind = gt_ranks[gt_names.index(name)]\n",
    "            image= images[cur_img_ind]\n",
    "\n",
    "            user_message_with_images.add_text(name)\n",
    "            user_message_with_images.add_img_array(image)\n",
    "        user_message_with_images.add_text(\"remember to format your answer in json\")\n",
    "        chat_history.add_user_msg(user_message_with_images)\n",
    "        output = call_model(chat_history, model=\"gpt-4o\")\n",
    "        print(output)\n",
    "        try:\n",
    "            try:\n",
    "                output = json.loads(output)\n",
    "            except:\n",
    "                # remove chatgpts format ```json\\n...\\b```\n",
    "                output = output.replace(\"```json\",\"\").replace(\"```\",\"\").strip()\n",
    "                output = json.loads(output)\n",
    "            explanation = output[\"info\"]\n",
    "            pred_rank_names = output[\"ranking\"]\n",
    "            pred_ranks = [pred_rank_names.index(name) for name in gt_names]\n",
    "        except:\n",
    "            pred_ranks = None \n",
    "            explanation = None\n",
    "\n",
    "\n",
    "        if pred_ranks is not None:\n",
    "            score = pearsonr([z+1 for z in gt_ranks], [z+1 for z in pred_ranks])\n",
    "            r_val, p_val = score[0], score[1]\n",
    "            score = r_val \n",
    "            p_vals.append(p_val)\n",
    "            three_scores.append(score)\n",
    "        else:\n",
    "            total_score = 0\n",
    "            three_scores= \"ERROR\"\n",
    "            p_vals = \"ERROR\"\n",
    "            explanation = \"ERROR\"\n",
    "            break \n",
    "        total_score = score + total_score\n",
    "    total_score = total_score/3\n",
    "    scores.append(total_score)\n",
    "    print(three_scores)\n",
    "    arrs.append(arr)\n",
    "\n",
    "    scores_text = ', '.join(f\"{x:.2f}\" for x in three_scores) if type(three_scores) == list else \"ERROR\"\n",
    "    p_vals_text = ', '.join(f\"{x:.4f}\" for x in p_vals) if type(p_vals) == list else \"ERROR\"\n",
    "    text =  (\n",
    "          f\"DESCRIPTION BEING JUDGED: {descriptions[i]}\\n\"\n",
    "          f\"SCORE (avg over three runs): {total_score:.3f}\\n\"\n",
    "          f\"(all scores for three runs: {scores_text})\\n\"\n",
    "          f\"PVALUE (three runs) {p_vals_text}\\n\"\n",
    "          f\"Model justification (final run):::\\n{explanation}\\n\"\n",
    "          f\"Model predicted order (final run) {pred_ranks}\\n\"\n",
    "          f\"Images are what was provided to final run\")\n",
    "    texts.append(text)\n",
    "\n",
    "    print(text)\n",
    " \n",
    "#print(scores, texts, arrs)\n",
    "sorted_lists = zip(*sorted(zip(scores, texts, arrs), reverse=True))\n",
    "sscores, sdescriptions, sarrs = [list(tup) for tup in sorted_lists]\n",
    "\n",
    "for d, a in zip(sdescriptions, sarrs):\n",
    "    pdf_content_ranking.append(d)\n",
    "    pdf_content_ranking.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "result = []\n",
    "for i in range(0, len(pdf_content_ranking), 2):\n",
    "    result.extend(pdf_content_ranking[i:i+2])  # Add the next two items\n",
    "    result.append('<pagebreak>')\n",
    "\n",
    "create_pdf(pdf_content, file_name=os.path.join(OUTPUT_FOLDER, \"gpt4o_autointerp.pdf\"))\n",
    "create_pdf(result, file_name=os.path.join(OUTPUT_FOLDER, \"gpt4o_autointerp_ranking.pdf\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
