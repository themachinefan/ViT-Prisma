{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths and so on\n",
    "PARQUEET_INPUT = r\"\"\n",
    "OUTPUT_FOLDER = r\"\"\n",
    "\n",
    "\n",
    "# parquet column names\n",
    "PATCH_IDX = \"patchIdx\"\n",
    "FEATURE_IDX = \"featureIdx\"\n",
    "IMAGE_IDX = \"imageIdx\"\n",
    "ACTIVATION_VALUE = \"activationValue\"\n",
    "LABEL = \"label\"\n",
    "TYPE = \"type\"\n",
    "LAYER_IDX = \"layerIdx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import torchvision \n",
    "import matplotlib.pyplot as plt\n",
    "from sae.basic_vision_api_call import UserMessage, ImageChatHistory, call_model\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os \n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(PARQUEET_INPUT)\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = load_dataset('Prisma-Multimodal/segmented-imagenet1k-subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, dataset, patch_size=32, width=224, height=224, return_label = True):\n",
    "        \"\"\"\n",
    "        dataset: A list of dictionaries, each dictionary corresponds to an image and its details\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform =  torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.Resize((224, 224)),\n",
    "                        torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Lambda(lambda img: img.permute(1, 2, 0))\n",
    "                        ])\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.return_label = return_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = self.transform(item['image'])\n",
    "        if self.return_label:\n",
    "            masks = item['masks']\n",
    "            labels = item['labels']  # Assuming labels are aligned with masks\n",
    "            \n",
    "            # Calculate the size of the reduced mask\n",
    "            num_patches = self.width // self.patch_size\n",
    "            label_array = [[[] for _ in range(num_patches)] for _ in range(num_patches)]\n",
    "            \n",
    "            for mask, label in zip(masks, labels):\n",
    "                # Resize and reduce the mask\n",
    "                mask = mask.resize((self.width, self.height))\n",
    "                mask_array = np.array(mask) > 0\n",
    "                reduced_mask = self.reduce_mask(mask_array)\n",
    "                \n",
    "                # Populate the label array based on the reduced mask\n",
    "                for i in range(num_patches):\n",
    "                    for j in range(num_patches):\n",
    "                        if reduced_mask[i, j]:\n",
    "                            label_array[i][j].append(label)\n",
    "            \n",
    "            # Convert label_array to a format suitable for tensor operations, if necessary\n",
    "            # For now, it's a list of lists of lists, which can be used directly in Python\n",
    "            \n",
    "            return image, label_array, idx\n",
    "        else:\n",
    "            return image, idx \n",
    "    \n",
    "\n",
    "    def reduce_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Reduce the mask size by dividing it into patches and checking if there's at least\n",
    "        one True value within each patch.\n",
    "        \"\"\"\n",
    "        # Calculate new height and width\n",
    "        new_h = mask.shape[0] // self.patch_size\n",
    "        new_w = mask.shape[1] // self.patch_size\n",
    "        \n",
    "        reduced_mask = np.zeros((new_h, new_w), dtype=bool)\n",
    "        \n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                patch = mask[i*self.patch_size:(i+1)*self.patch_size, j*self.patch_size:(j+1)*self.patch_size]\n",
    "                reduced_mask[i, j] = np.any(patch)  # Set to True if any value in the patch is True\n",
    "        \n",
    "        return reduced_mask\n",
    "\n",
    "\n",
    "\n",
    "patch_label_dataset = PatchDataset(default_dataset['train'], return_label=False)\n",
    "im, idx = patch_label_dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# for a given feature (and 'type'), find all images and heatmaps associated with it. \n",
    "def get_overlays_and_images(feature_id, type_label=\"top\", num_patches_w=7, low=0.25, high=0.75, display=False):\n",
    "    filtered_df = df[df[FEATURE_IDX] == feature_id]\n",
    "    \n",
    "    # Group by both imageIdx and type\n",
    "    grouped = filtered_df.groupby([IMAGE_IDX, TYPE])\n",
    "    \n",
    "    images = []\n",
    "    overlays = []\n",
    "    for (image_idx, type_), group in grouped:\n",
    "        if type_!=type_label:\n",
    "            continue\n",
    "        activations = group[ACTIVATION_VALUE].values\n",
    "        # get heatmap\n",
    "        heatmap = activations[1:].reshape((num_patches_w,num_patches_w))\n",
    "        image, _ = patch_label_dataset[int(image_idx)]\n",
    "        image = image.detach().cpu().numpy()\n",
    "        # normalize\n",
    "        heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "        # clip\n",
    "        heatmap[np.logical_and(0.1 <= heatmap, heatmap <= low)] = 0.1\n",
    "        heatmap[heatmap >= high] = 1\n",
    "        # upscale\n",
    "        heatmap = np.repeat(np.repeat(heatmap, 224//num_patches_w, axis=0), 224//num_patches_w, axis=1)\n",
    "        #create 3rd dim\n",
    "        heatmap = np.stack([heatmap]*3, axis=-1)\n",
    "        overlay = image*heatmap\n",
    "\n",
    "        if display:\n",
    "            plt.imshow(overlay)\n",
    "            plt.show()\n",
    "        images.append(np.uint8(255*image))\n",
    "        overlays.append(np.uint8(255*overlay))\n",
    "    return overlays, images\n",
    "\n",
    "\n",
    "_,_ = get_overlays_and_images(df[FEATURE_IDX].iloc[0], display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some hacky code to generate a pdf \n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.platypus import Image as ReportLabImage\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from PIL import Image \n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resize_image(image, max_width, max_height):\n",
    "    # Calculate the new dimensions maintaining the aspect ratio\n",
    "    width_percent = max_width / float(image.size[0])\n",
    "    height_percent = max_height / float(image.size[1])\n",
    "    aspect_ratio = min(width_percent, height_percent)\n",
    "\n",
    "    # New dimensions\n",
    "    width = int((float(image.size[0]) * float(aspect_ratio)))\n",
    "    height = int((float(image.size[1]) * float(aspect_ratio)))\n",
    "\n",
    "    return image.resize((width, height), Image.ANTIALIAS)\n",
    "def create_pdf(content_list, file_name='output.pdf'):\n",
    "    doc = SimpleDocTemplate(file_name, pagesize=letter)\n",
    "    elements = []\n",
    "    style_sheet = getSampleStyleSheet()\n",
    "\n",
    "    #TODO figure out how to do this better.. \n",
    "    max_image_width = 456#letter[0] - 50\n",
    "    max_image_height = 636#letter[1] - 50\n",
    "    for item in content_list:\n",
    "        if isinstance(item, str):\n",
    "            # Add text\n",
    "            item = item.replace('\\n', '<br/>')\n",
    "\n",
    "            elements.append(Paragraph(item, style_sheet['BodyText']))\n",
    "            elements.append(Spacer(1, 12))  # Add space after paragraph\n",
    "\n",
    "        elif isinstance(item, np.ndarray):\n",
    "            # Convert numpy array to list of lists and create a table\n",
    "          #  item = item[:,:,0:3]\n",
    "            image = Image.fromarray(item.astype('uint8'), 'RGB')\n",
    "            if image.size[0] > max_image_width or image.size[1] > max_image_height:\n",
    "                image = resize_image(image, max_image_width, max_image_height)\n",
    "            image_buffer = io.BytesIO()\n",
    "            image.save(image_buffer, format='PNG')\n",
    "            image_buffer.seek(0)\n",
    "            img = ReportLabImage(image_buffer)\n",
    "            elements.append(img)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Content list must contain only strings and numpy arrays.\")\n",
    "\n",
    "    doc.build(elements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = df[FEATURE_IDX].unique()\n",
    "\n",
    "print(len(all_features))\n",
    "from io import BytesIO\n",
    "\n",
    "def plot_grid_of_arrays(arrays):\n",
    "    n = len(arrays)\n",
    "    grid_size_w = int(np.ceil(np.sqrt(n)))\n",
    "    grid_size_h = int(np.ceil(n/grid_size_w))\n",
    "    _, axes = plt.subplots(grid_size_h, grid_size_w, figsize=(grid_size_w*2, grid_size_h*2))\n",
    "    \n",
    "    for ax, array in zip(axes.flat, arrays):\n",
    "        ax.imshow(array, cmap='gray')  # Assuming the arrays are 2D grayscale images\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for ax in axes.flat[n:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', transparent=False)\n",
    "    buf.seek(0)\n",
    "\n",
    "    image = Image.open(buf)\n",
    "    image_rgb = image.convert('RGB')  # Convert to RGB\n",
    "    image_np = np.array(image_rgb)\n",
    "\n",
    "    buf.close()\n",
    "    plt.show()\n",
    "\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_MESSAGE = \"The following are images of a TinyCLIP neuron's activations. We want to determine what the neuron is responding too. Areas of the image that are LESS relevant are darker. \\\n",
    "      Focus on the visible areas. Notice small details and patterns across the images. Respond in a concise, descriptive phrase exactly what it's selecting for, and \\\n",
    "            try not to use words neuron, activation, or image, or focus/selective/etc, because that will be redundant for our purposes. \\\n",
    "            Keep your words simple but be pretty specific and granular. For example, instead of saying 'face,' note the \\\n",
    "              specific parts of a face, like ears, neck, etc.\"\n",
    "num_images = 10\n",
    "num_features = 25 # len(all_features)\n",
    "pdf_content = []\n",
    "for feature_id in all_features[0:num_features]:\n",
    "    top_overlays, top_images = get_overlays_and_images(feature_id)\n",
    "\n",
    "    chat_history = ImageChatHistory()\n",
    "\n",
    "    chat_history.add_system_msg(SYSTEM_MESSAGE)\n",
    "\n",
    "\n",
    "    user_message_with_images = UserMessage()\n",
    "\n",
    "    user_message_with_images.add_text(\"Here are the images:\")\n",
    "\n",
    "    for i in range(num_images):\n",
    "        user_message_with_images.add_img_array(top_overlays[i])\n",
    "\n",
    "    chat_history.add_user_msg(user_message_with_images)\n",
    "\n",
    "\n",
    "    display_image = plot_grid_of_arrays(top_overlays[0:num_images])\n",
    "\n",
    "    description = call_model(chat_history, model=\"gpt-4o\")\n",
    "    strr = f\"----------------\\n{feature_id}\\n---------------\\n=====Model output:::\\n{description}\\n\"\n",
    "    print(strr)\n",
    "\n",
    "    pdf_content.append(display_image)\n",
    "    pdf_content.append(strr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy hack\n",
    "to_delete = []\n",
    "for i in range(1,50,2):\n",
    "   # print(pdf_content[i])\n",
    "    feature_id = all_features[int((i-1)/2)]\n",
    "    if \"'message':\" in pdf_content[i]:\n",
    "        to_delete.append(i-1)\n",
    "        to_delete.append(i)\n",
    "\n",
    "to_delete = to_delete[::-1]\n",
    "\n",
    "for thing in to_delete:\n",
    "    del pdf_content[thing]\n",
    "       # pdf_content[i] = strr\n",
    "\n",
    "        \n",
    "# for feature_id in all_features[0:num_features]:\n",
    "\n",
    "\n",
    "#     top_overlays, top_images = get_overlays_and_images(feature_id)\n",
    "\n",
    "#     chat_history = ImageChatHistory()\n",
    "\n",
    "#     chat_history.add_system_msg(SYSTEM_MESSAGE)\n",
    "\n",
    "\n",
    "#     user_message_with_images = UserMessage()\n",
    "\n",
    "#     user_message_with_images.add_text(\"Here are the images:\")\n",
    "\n",
    "#     for i in range(num_images):\n",
    "#         user_message_with_images.add_img_array(top_overlays[i])\n",
    "\n",
    "#     chat_history.add_user_msg(user_message_with_images)\n",
    "\n",
    "\n",
    "#     display_image = plot_grid_of_arrays(top_overlays[0:num_images])\n",
    "\n",
    "#     description = call_model(chat_history, model=\"gpt-4o\")\n",
    "#     strr = f\"----------------\\n{feature_id}\\n---------------\\n=====Model output:::\\n{description}\\n\"\n",
    "#     print(strr)\n",
    "\n",
    "#     pdf_content.append(display_image)\n",
    "#     pdf_content.append(strr)\n",
    "# print(len(pdf_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pdf_content)\n",
    "create_pdf(pdf_content, file_name=os.path.join(OUTPUT_FOLDER, \"gpt4o_autointerp.pdf\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
