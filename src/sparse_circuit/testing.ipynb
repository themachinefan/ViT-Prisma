{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "saved_nodes_path = r\"F:\\ViT-Prisma_fork\\data\\circuit_output\\praneet\\testing_stuff_2_nodes.pt\"\n",
    "saved_edges_paths = r\"F:\\ViT-Prisma_fork\\data\\circuit_output\\praneet\\testing_stuff_2_edges.pt\"\n",
    "saved_features_path = r\"F:\\ViT-Prisma_fork\\data\\circuit_output\\praneet\\testing_stuff_2_features.pt\"\n",
    "all_nodes= torch.load(saved_nodes_path)\n",
    "all_edges= torch.load(saved_edges_paths)\n",
    "node_mask = torch.load(saved_features_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_indices = {}\n",
    "node_values = {}\n",
    "node_indices_alt = {}\n",
    "node_values_alt = {} \n",
    "for hook_point, all_nodes_values in all_nodes.items():\n",
    "   # print(hook_point)\n",
    "  #  print(\"all node values for\", hook_point, \"shape: \", nodes.shape)\n",
    "    good_nodes = node_mask[hook_point].nonzero()[:,2]\n",
    "    node_indices[hook_point] = list(set(good_nodes.tolist()))\n",
    "    node_values[hook_point] = all_nodes_values[node_indices[hook_point]].tolist()\n",
    "\n",
    "    for src_hook_point in all_edges.keys():\n",
    "        for dst_hook_point in all_edges[src_hook_point].keys():\n",
    "            if dst_hook_point == hook_point:\n",
    "    \n",
    "                edges = all_edges[src_hook_point][dst_hook_point]\n",
    "\n",
    "            \n",
    "                node_indices_alt[dst_hook_point] = edges.coalesce().indices()[0].tolist()\n",
    "                node_values_alt[dst_hook_point] = all_nodes_values[node_indices_alt[dst_hook_point]].tolist()\n",
    "\n",
    "node_indices = node_indices_alt\n",
    "node_values = node_values_alt\n",
    "\n",
    "\n",
    "custom_node_indices = {}\n",
    "custom_node_values = {}\n",
    "for hook_point, results in all_nodes.items():\n",
    "\n",
    "    results = results.abs()\n",
    "    mean = torch.mean(results)\n",
    "    std = torch.std(results)\n",
    "\n",
    "    num_std = 3  # For values greater than one standard deviation above the mean\n",
    "    max_amount = 20 # just to keep the analysis smaller... \n",
    "\n",
    "    # Find indices where A > mean + num_std * std\n",
    "    indices = torch.nonzero(results > mean + num_std * std).squeeze()\n",
    "    values = results[indices]\n",
    "\n",
    "    if max_amount is not None:\n",
    "        if len(values)> max_amount:\n",
    "            og_amount = len(values)   \n",
    "            values, top_k_indices = torch.topk(values, max_amount, largest=True)\n",
    "            indices = indices[top_k_indices]\n",
    "            print(\"removed\", og_amount-max_amount, 'to save time')\n",
    "\n",
    "    values, sorted_indices = torch.sort(values, descending=True)\n",
    "    \n",
    "    custom_node_indices[hook_point] = indices[sorted_indices].tolist()\n",
    "    custom_node_values = values.tolist()\n",
    "\n",
    "\n",
    "node_indices = custom_node_indices\n",
    "node_values = custom_node_values\n",
    "# edges = {}\n",
    "# for src_hook_point in all_edges.keys():\n",
    "#     edges[src_hook_point] = {}\n",
    "#     for dst_hook_point, cur_edges in all_edges[src_hook_point].items():\n",
    "#         edges[src_hook_point][dst_hook_point] = {}\n",
    "\n",
    "#         for dst_ind in node_indices[dst_hook_point]:\n",
    "            \n",
    "#             values = cur_edges[dst_ind][node_indices[src_hook_point]].tolist()\n",
    "\n",
    "#             #TODO sparse tensor instead \n",
    "#             for val, src_ind in zip(values,node_indices[src_hook_point] ):\n",
    "\n",
    "#                 if src_ind not in  edges[src_hook_point][dst_hook_point].keys():\n",
    "#                      edges[src_hook_point][dst_hook_point][src_ind] = {} \n",
    "                \n",
    "#                 edges[src_hook_point][dst_hook_point][src_ind][dst_ind] = val \n",
    "#print(edges)\n",
    "            \n",
    "\n",
    "       # print(\"===\")\n",
    "       # print(len(dst_indices))\n",
    "       # print(len(node_indices[dst_hook_point]))\n",
    "\n",
    "        \n",
    "        # print(f\"edges between nodes from {src_hook_point} to {dst_hook_point} (I might have src dst backward)\", edges.shape)\n",
    "        # downstream_indices = edges.coalesce().indices()[0]\n",
    "        # downstream_values = edges.coalesce().values()\n",
    "        # print(downstream_values.shape)\n",
    "        # print(downstream_indices)\n",
    "        # print(\"---\")\n",
    "        # for ind in downstream_indices:\n",
    "            \n",
    "        #     ind = ind.item()\n",
    "        #     print(edges[ind].shape)\n",
    "        #     values = edges[ind]\n",
    "        #     print(values.mean(), values.std())\n",
    "        #     good_upstream_indices = (values.abs() > edge_threshold).nonzero()\n",
    "        #     print('found ', len(good_upstream_indices))\n",
    "\n",
    "\n",
    "        # print(edges.coalesce().indices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "import einops\n",
    "\n",
    "from typing import List \n",
    "\n",
    "import re \n",
    "from PIL import Image\n",
    "from sparse_circuit.demo import setup, get_imagenet_val_dataset, get_imagenet_val_dataset_visualize\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    is_cls_list: List[bool],\n",
    "    top_k: int = 10\n",
    ") -> Dict[int, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        feature_categories: Categories of the features\n",
    "        top_k: Number of top activations to return per feature\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "    _, cache = model.run_with_cache(images, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "    \n",
    "    layer_activations = cache[sparse_autoencoder.cfg.hook_point]\n",
    "    batch_size, seq_len, _ = layer_activations.shape\n",
    "    flattened_activations = einops.rearrange(layer_activations, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    \n",
    "    sae_input = flattened_activations - sparse_autoencoder.b_dec\n",
    "    feature_activations = einops.einsum(sae_input, encoder_weights, \"... d_in, d_in n -> ... n\") + encoder_biases\n",
    "    feature_activations = torch.nn.functional.relu(feature_activations)\n",
    "    \n",
    "    reshaped_activations = einops.rearrange(feature_activations, \"(batch seq) d_in -> batch seq d_in\", batch=batch_size, seq=seq_len)\n",
    "    cls_token_activations = reshaped_activations[:, 0, :]\n",
    "    mean_image_activations = reshaped_activations.mean(1)\n",
    "\n",
    "    top_activations = {}\n",
    "    for i, (feature_id, is_cls) in enumerate(zip(feature_ids, is_cls_list)):\n",
    "        if is_cls:\n",
    "            top_values, top_indices = cls_token_activations[:, i].topk(top_k)\n",
    "        else:\n",
    "            top_values, top_indices = mean_image_activations[:, i].topk(top_k)\n",
    "        top_activations[feature_id] = (top_indices, top_values)\n",
    "    \n",
    "    return top_activations\n",
    "\n",
    "def find_top_activations(\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    interesting_features_indices: List[int],\n",
    "    is_cls_list: List[bool],\n",
    "    top_k: int = 16,\n",
    "    max_samples= 50_000,\n",
    "    batch_size = 54, \n",
    ") -> Dict[int, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Find the top activations for interesting features across the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        val_dataloader: Validation data loader\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        interesting_features_indices: Indices of interesting features\n",
    "        interesting_features_category: Categories of interesting features\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_values, top_indices)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    top_activations = {i: (None, None) for i in interesting_features_indices}\n",
    "\n",
    "    print(sparse_autoencoder.b_enc.shape, sparse_autoencoder.W_enc.shape)\n",
    "    print(interesting_features_indices)\n",
    "   # print(sparse_autoencoder.b)\n",
    "    #sparse_autoencoder.b_enc =  sparse_autoencoder.b_enc.to('cpu')\n",
    "    encoder_biases = sparse_autoencoder.b_enc[interesting_features_indices]\n",
    "    encoder_weights = sparse_autoencoder.W_enc[:, interesting_features_indices]\n",
    "\n",
    "    processed_samples = 0\n",
    "    for batch_images, _, batch_indices in tqdm(val_dataloader, total=max_samples // batch_size):\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_indices = batch_indices.to(device)\n",
    "        batch_size = batch_images.shape[0]\n",
    "\n",
    "        batch_activations = compute_feature_activations(\n",
    "            batch_images, model, sparse_autoencoder, encoder_weights, encoder_biases,\n",
    "            interesting_features_indices, is_cls_list, top_k\n",
    "        )\n",
    "\n",
    "        for feature_id in interesting_features_indices:\n",
    "            new_indices, new_values = batch_activations[feature_id]\n",
    "            new_indices = batch_indices[new_indices]\n",
    "            \n",
    "            if top_activations[feature_id][0] is None:\n",
    "                top_activations[feature_id] = (new_values, new_indices)\n",
    "            else:\n",
    "                combined_values = torch.cat((top_activations[feature_id][0], new_values))\n",
    "                combined_indices = torch.cat((top_activations[feature_id][1], new_indices))\n",
    "                _, top_k_indices = torch.topk(combined_values, top_k)\n",
    "                top_activations[feature_id] = (combined_values[top_k_indices], combined_indices[top_k_indices])\n",
    "\n",
    "        processed_samples += batch_size\n",
    "        if processed_samples >= max_samples:\n",
    "            break\n",
    "\n",
    "    return {i: (values.detach().cpu(), indices.detach().cpu()) \n",
    "            for i, (values, indices) in top_activations.items()}\n",
    "\n",
    "torch.no_grad()\n",
    "def get_heatmap(\n",
    "          image,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "          device,\n",
    "): \n",
    "    image = image.to(device)\n",
    "    _, cache = model.run_with_cache(image.unsqueeze(0), names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "\n",
    "    post_reshaped = einops.rearrange(sae.run_time_activation_norm_fn_in(cache[sparse_autoencoder.cfg.hook_point]), \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"x d_in, d_in -> x\",\n",
    "        )\n",
    "    return acts \n",
    "@torch.no_grad()\n",
    "def get_heatmap_batch(\n",
    "          images,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "          device,\n",
    "): \n",
    "    image = torch.stack(images, dim=0).to(device)\n",
    "    _, cache = model.run_with_cache(image, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "\n",
    "    post_reshaped = sae.run_time_activation_norm_fn_in(cache[sparse_autoencoder.cfg.hook_point])\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"b x d_in, d_in -> b x\",\n",
    "        )\n",
    "    return acts \n",
    "     \n",
    "def image_patch_heatmap(activation_values,image_size=224, pixel_num=14):\n",
    "    activation_values = activation_values.detach().cpu().numpy()\n",
    "    activation_values = activation_values[1:]\n",
    "    activation_values = activation_values.reshape(pixel_num, pixel_num)\n",
    "\n",
    "    # Create a heatmap overlay\n",
    "    heatmap = np.zeros((image_size, image_size))\n",
    "    patch_size = image_size // pixel_num\n",
    "\n",
    "    for i in range(pixel_num):\n",
    "        for j in range(pixel_num):\n",
    "            heatmap[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = activation_values[i, j]\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "\n",
    "    # Removing axes\n",
    "#TODO clean this up and do a batch version!\n",
    "def visualize_top_activating_images(save_folder, root_name, model, sae, top_activations_per_feature,feature_ids, attrib_values, dataset, dataset_visualize, device, patch_size=32):\n",
    "    importance = 0\n",
    "    for feature_id, attrib_value in tqdm(zip(feature_ids, attrib_values)):\n",
    "        importance += 1\n",
    "        max_vals, max_inds = top_activations_per_feature[feature_id]\n",
    "        images = []\n",
    "        model_images = []\n",
    "        for bid in max_inds:\n",
    "\n",
    "            image, _, image_ind = dataset_visualize[bid]\n",
    "\n",
    "            assert image_ind.item() == bid\n",
    "            images.append(image)\n",
    "\n",
    "            model_image, _, _ = dataset[bid]\n",
    "            model_images.append(model_image)\n",
    "        \n",
    "        grid_size = int(np.ceil(np.sqrt(len(images))))\n",
    "        fig, axs = plt.subplots(int(np.ceil(len(images)/grid_size)), grid_size, figsize=(15, 15))\n",
    "        name=  f\"Feature: {feature_id} Node val: {attrib_value}\"\n",
    "        fig.suptitle(name)#, y=0.95)\n",
    "        for ax in axs.flatten():\n",
    "            ax.axis('off')\n",
    "        complete_bid = []\n",
    "        heatmaps = get_heatmap_batch(model_images,model,sae, feature_id,device )\n",
    "\n",
    "        for i, (image_tensor, val, bid, heatmap) in enumerate(zip(images, max_vals,max_inds,heatmaps )):\n",
    "            if bid in complete_bid:\n",
    "                continue \n",
    "            complete_bid.append(bid)\n",
    "\n",
    "\n",
    "            row = i // grid_size\n",
    "            col = i % grid_size\n",
    "            heatmap = image_patch_heatmap(heatmap, pixel_num=224//patch_size)\n",
    "\n",
    "            display = image_tensor.numpy().transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "            axs[row, col].imshow(display)\n",
    "            axs[row, col].imshow(heatmap, cmap='viridis', alpha=0.3)  # Overlaying the heatmap\n",
    "            axs[row, col].set_title(f\"{val.item():0.03f}\")  \n",
    "            axs[row, col].axis('off')  \n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(save_folder, f\"{root_name}_{feature_id}.jpg\"), format=\"jpg\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run setup\n",
    "device = \"cuda\"\n",
    "imagenet_dataset_path = r\"F:/prisma_data/imagenet-object-localization-challenge\"\n",
    "\n",
    "output_folder = r\"F:\\ViT-Prisma_fork\\data\\circuit_output\\praneet_custom\"\n",
    "im_folder = os.path.join(output_folder, 'images')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(im_folder, exist_ok=True)\n",
    "model, saes, model_name = setup(device, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_imagenet_val_dataset(imagenet_dataset_path)\n",
    "visualize_dataset = get_imagenet_val_dataset_visualize(imagenet_dataset_path)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader =  DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Get images for each feature\n",
    "hook_points_ordered = list(node_indices.keys())[::-1] # I just want to start with later layers\n",
    "for hook_point in hook_points_ordered:\n",
    "    sae = saes[hook_point]\n",
    "\n",
    "    feature_ids = node_indices[hook_point]\n",
    "    feature_vals = node_values[hook_point]\n",
    "\n",
    "    max_feature = all_nodes[hook_point].shape[0] -1 \n",
    "    print('max', max_feature)\n",
    "    pruned_feature_ids = []\n",
    "    pruned_feature_vals = []\n",
    "    error_features = []\n",
    "    error_vals = []\n",
    "    for i, v in zip(feature_ids, feature_vals):\n",
    "        if i != max_feature:\n",
    "            pruned_feature_ids.append(i)\n",
    "            pruned_feature_vals.append(v)\n",
    "        else:\n",
    "            error_features.append(i)\n",
    "            error_vals.append(v)\n",
    "    if pruned_feature_ids:\n",
    "        print(\"HI\", pruned_feature_ids)\n",
    "        top_activations_per_feature = find_top_activations(\n",
    "            dataloader, model, sae,\n",
    "            pruned_feature_ids, [False]*len(pruned_feature_ids), batch_size=batch_size, top_k=16, max_samples=50_000,\n",
    "        )\n",
    "\n",
    "        root_name = re.sub(r'\\b\\d+\\b', lambda x: f\"{int(x.group()):02d}\", hook_point)\n",
    "        visualize_top_activating_images(im_folder, root_name, model, sae, top_activations_per_feature, pruned_feature_ids, pruned_feature_vals, dataset, visualize_dataset, device)\n",
    "\n",
    "    if error_features:\n",
    "        for e in error_features:\n",
    "            # make a dummy image!\n",
    "\n",
    "            # Create a black image of size 100x100\n",
    "            black_image = Image.new(\"RGB\", (100, 100), (0, 0, 0))\n",
    "\n",
    "            #TODO add text? make better shape\n",
    "            black_image_path =  os.path.join(im_folder, f\"{root_name}_{e}.jpg\")\n",
    "                                            \n",
    "        black_image.save(black_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get edges\n",
    "edges = {}\n",
    "for src_hook_point in all_edges.keys():\n",
    "    edges[src_hook_point] = {}\n",
    "    for dst_hook_point, cur_edges in all_edges[src_hook_point].items():\n",
    "        edges[src_hook_point][dst_hook_point] = {}\n",
    "\n",
    "        for dst_ind in node_indices[dst_hook_point]:\n",
    "            \n",
    "            values = cur_edges[dst_ind][node_indices[src_hook_point]].tolist()\n",
    "\n",
    "            #TODO sparse tensor instead \n",
    "            for val, src_ind in zip(values,node_indices[src_hook_point] ):\n",
    "\n",
    "                if src_ind not in  edges[src_hook_point][dst_hook_point].keys():\n",
    "                     edges[src_hook_point][dst_hook_point][src_ind] = {} \n",
    "                \n",
    "                edges[src_hook_point][dst_hook_point][src_ind][dst_ind] = val \n",
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
